{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport json\nimport requests\nurl = 'https://raw.githubusercontent.com/brearenee/NLP-Project/main/dataset/StarTrekDialogue_v2.json'\nresponse = requests.get(url)\n\n##This CodeBlock is thanks to ChatGPT :-) \nif response.status_code == 200:\n    json_data = json.loads(response.text)\n    lines = []\n    characters = []\n    episodes = []\n  \n    # extract the information from the JSON file for the \"TNG\" series\n    for series_name, series_data in json_data.items():\n        if series_name == \"TNG\": \n            for episode_name, episode_data in series_data.items():\n                for character_name, character_lines in episode_data.items():\n                    for line_text in character_lines:\n                        lines.append(line_text)\n                        characters.append(character_name)\n                        episodes.append(episode_name)\n                     \n    # Create a DataFrame from the extracted data\n    df = pd.DataFrame({\n        'Line': lines,\n        'Character': characters,\n    })\n\n    # Remove duplicate lines, keeping the first occurrence (preserving the original order)\n    df = df.drop_duplicates(subset='Line', keep='first')\n\n    # Reset the index of the DataFrame\n    df.reset_index(drop=True, inplace=True)\n\nelse:\n    print(f\"Failed to retrieve data. Status code: {response.status_code}\")\n    \n    \n##Remove Outliers (Characters with less than 1000 lines)\ncharacter_counts = df['Character'].value_counts()\ncharacters_to_remove = character_counts[character_counts < 1000].index\ndf = df[~df['Character'].isin(characters_to_remove)]\n\n##Print Value Count. \nprint(df['Character'].value_counts())\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-04T05:20:32.419384Z","iopub.execute_input":"2023-12-04T05:20:32.419835Z","iopub.status.idle":"2023-12-04T05:20:33.139378Z","shell.execute_reply.started":"2023-12-04T05:20:32.419803Z","shell.execute_reply":"2023-12-04T05:20:33.138254Z"},"trusted":true},"execution_count":63,"outputs":[{"name":"stdout","text":"Character\nPICARD     10798\nRIKER       6454\nDATA        5699\nLAFORGE     4111\nWORF        3185\nCRUSHER     2944\nTROI        2856\nWESLEY      1206\nName: count, dtype: int64\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# BERT","metadata":{}},{"cell_type":"code","source":"#https://www.analyticsvidhya.com/blog/2021/12/multiclass-classification-using-transformers/\n\n\n#Split the data \nfrom sklearn.model_selection import train_test_split\ntrain_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n\n#Converting our Character column into Categorical data\nencoded_dict = {'PICARD':0,'RIKER':1, 'DATA':2, 'LAFORGE':3, \n                'WORF':4, 'CRUSHER':5, 'TROI':6,'WESLEY':7}\ntrain_df['Character'] = train_df.Character.map(encoded_dict)\nval_df['Character'] = val_df.Character.map(encoded_dict)\n\ntrain_df.head()\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-12-04T05:20:33.141071Z","iopub.execute_input":"2023-12-04T05:20:33.141432Z","iopub.status.idle":"2023-12-04T05:20:33.167170Z","shell.execute_reply.started":"2023-12-04T05:20:33.141402Z","shell.execute_reply":"2023-12-04T05:20:33.165998Z"},"trusted":true},"execution_count":64,"outputs":[{"execution_count":64,"output_type":"execute_result","data":{"text/plain":"                                                    Line  Character\n41033  The upper portion of the apparatus seems\\r to ...          2\n19647  how long before they cross over into Federatio...          1\n20640  You know, this might work. We can't change the...          3\n16952  And you conclude because of this that I am imp...          2\n11530  I was thinking the same thing about you. In al...          1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Line</th>\n      <th>Character</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>41033</th>\n      <td>The upper portion of the apparatus seems\\r to ...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>19647</th>\n      <td>how long before they cross over into Federatio...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>20640</th>\n      <td>You know, this might work. We can't change the...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>16952</th>\n      <td>And you conclude because of this that I am imp...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>11530</th>\n      <td>I was thinking the same thing about you. In al...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"val_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-04T05:20:33.168415Z","iopub.execute_input":"2023-12-04T05:20:33.168729Z","iopub.status.idle":"2023-12-04T05:20:33.177829Z","shell.execute_reply.started":"2023-12-04T05:20:33.168702Z","shell.execute_reply":"2023-12-04T05:20:33.176801Z"},"trusted":true},"execution_count":65,"outputs":[{"execution_count":65,"output_type":"execute_result","data":{"text/plain":"                                                    Line  Character\n48560  If you had to give this feeling a name, what w...          6\n47262  I'm sure you all understand that in light of w...          5\n7425   Somehow, and there is limited information on t...          2\n29414  But our sensors were malfunctioning. Our probe...          1\n33815  The signal ended abruptly at oh four five five...          1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Line</th>\n      <th>Character</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>48560</th>\n      <td>If you had to give this feeling a name, what w...</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>47262</th>\n      <td>I'm sure you all understand that in light of w...</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>7425</th>\n      <td>Somehow, and there is limited information on t...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>29414</th>\n      <td>But our sensors were malfunctioning. Our probe...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>33815</th>\n      <td>The signal ended abruptly at oh four five five...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from tensorflow.keras.utils import to_categorical\n\ny_train = to_categorical(train_df.Character)\ny_test = to_categorical(val_df.Character)\n\n#We have successfully processed our Sentiment column( target); \n#now, it’s time to process our input text data using a tokenizer.","metadata":{"execution":{"iopub.status.busy":"2023-12-04T05:20:33.179631Z","iopub.execute_input":"2023-12-04T05:20:33.179974Z","iopub.status.idle":"2023-12-04T05:20:33.190161Z","shell.execute_reply.started":"2023-12-04T05:20:33.179945Z","shell.execute_reply":"2023-12-04T05:20:33.188933Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"import transformers\n\n#Loading Model and Tokenizer from the transformers package \n\nfrom transformers import AutoTokenizer,TFBertModel\n#bert-base-uncased is another possible one\ntokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n#TFBertModel = pretrained BERT model for Tensor Flow\nbert = TFBertModel.from_pretrained('bert-base-cased')\n\n#Input Data Modeling\n\n#Before training, we need to convert the input textual data into \n#BERT’s input data format using a tokenizer.\n#Since we have loaded bert-base-cased, \n#so tokenizer will also be Bert-base-cased.\n# Tokenize the input (takes some time) \n# here tokenizer using from bert-base-cased\nx_train = tokenizer(\n    text=train_df.Line.tolist(),\n    add_special_tokens=True,\n    max_length=70,\n    truncation=True,\n    padding=True, \n    return_tensors='tf',\n    return_token_type_ids = False,\n    return_attention_mask = True,\n    verbose = True)\nx_test = tokenizer(\n    text=val_df.Line.tolist(),\n    add_special_tokens=True,\n    max_length=50,\n    truncation=True,\n    padding=True, \n    return_tensors='tf',\n    return_token_type_ids = False,\n    return_attention_mask = True,\n    verbose = True)\n\n\n#Hereafter data modelling, the tokenizer will return a dictionary (x_train) containing ‘Input_ids’, ‘attention_mask’ as key for their respective\n#data.\n\ninput_ids = x_train['input_ids']\nattention_mask = x_train['attention_mask']","metadata":{"execution":{"iopub.status.busy":"2023-12-04T05:20:33.191704Z","iopub.execute_input":"2023-12-04T05:20:33.192062Z","iopub.status.idle":"2023-12-04T05:20:39.506056Z","shell.execute_reply.started":"2023-12-04T05:20:33.192033Z","shell.execute_reply":"2023-12-04T05:20:39.504811Z"},"trusted":true},"execution_count":67,"outputs":[{"name":"stderr","text":"Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nAll the weights of TFBertModel were initialized from the PyTorch model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Model Building","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.initializers import TruncatedNormal\nfrom tensorflow.keras.losses import CategoricalCrossentropy\nfrom tensorflow.keras.metrics import CategoricalAccuracy\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.layers import Input, Dense\n\nmax_len = 70\ninput_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\ninput_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"attention_mask\")\nembeddings = bert(input_ids,attention_mask = input_mask)[0] \nout = tf.keras.layers.GlobalMaxPool1D()(embeddings)\nout = Dense(128, activation='relu')(out)\nout = tf.keras.layers.Dropout(0.1)(out)\nout = Dense(32,activation = 'relu')(out)\ny = Dense(6,activation = 'sigmoid')(out)\nmodel = tf.keras.Model(inputs=[input_ids, input_mask], outputs=y)\nmodel.layers[2].trainable = True","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Compilation\n\nDefining learning parameters and compiling the model.","metadata":{}},{"cell_type":"code","source":"\n\nThis article was published as a part of the Data Science Blogathon\nIntroduction\n\nIn the last article, we have discussed implementing the BERT model using the TensorFlow hub; you can read it here. Implementing BERT using the TensorFlow hub was tedious since we had to perform every step from scratch. First, we build our tokenizer, then design a function to process our data, and then develop our model for training.\n\nhere hugging face transformers package make implementation easier\n\nThis article will discuss the latest method to implement BERT or any other state of art model in the most accessible steps using the Transformers library\n\nfor a detailed explanation on BERT. Read my last article.\nImplementation of BERT using hugging face transformers library\n\nhugging face is an NLP-focused startup that provides a wide variety of solutions in NLP for TensorFlow and PyTorch.\n\nThe Transformers library contains more than 30 pre-trained models and 100 languages, along with 8 major architectures for natural language understanding (NLU) and natural language generation (NLG):\nDataHour: Democratising AI Deployment\n\nDate: 7 Dec   Time: 7 PM – 8 PM IST\n\n    BERT (from Google);\n    GPT-2 (from OpenAI);\n    GPT (from OpenAI);\n    Transformer-XL (from Google/CMU);\n    XLNet (from Google/CMU);\n    RoBERTa (from Facebook);\n    XLM (from Facebook);\n    DistilBERT (from HuggingFace).\n\nThe hugging face Transformers library required TensorFlow or PyTorch to load models, and it can train SOTA models in only a few lines of code and pre-process our data in only a few lines of code. The hugging face transformers library gives you the benefit of using pre-trained language models without requiring a vast and costly computational infrastructure and with simple implementation. Most State-of-the-Art models(SOTA) are provided directly and made available in the library in PyTorch and TensorFlow transparently and interchangeably. It works as an API in some sense.\n\n    Loading the Dataset\n    Pre-processing the raw data\n    Getting BERT Pre-trained model and its tokenizer\n    Training and evaluation\n    Prediction Pipeline\n\nLoading the Dataset\n\nThe dataset we are using the Emotions dataset for NLP.\n\nThis dataset contains text and their respective emotions, and it has train-data, test-data, and validation data.\n\n‘i was feeling listless from the need of new things, something different; sadness.’\n\nPython Code:\nLoading the dataset | Multiclass Classification Using Transformers\nSource: Local\nConverting our Sentiment column into Categorical data\n\nMapping sentiments label with some numbers using a python dictionary and then convert them into a categorical column using to_categorical.\n\nencoded_dict = {‘anger’:0,’fear’:1, ‘joy’:2, ‘love’:3, ‘sadness’:4, ‘surprise’:5}\ndf_train[‘Sentiment’] = df_train.Sentiment.map(encoded_dict)\ndf_test[‘Sentiment’] = df_train.Sentiment.map(encoded_dict)\n\nimporting to_categorical class from utils:\n\nfrom tensorflow.keras.utils import to_categorical\n\nconverting our integer coded Sentiment column into categorical data(matrix)\n\ny_train = to_categorical(df_train.Sentiment)\ny_test = to_categorical(df_test.Sentiment)\n\nConverting our Sentiment column into Categorical data\nSource: Local\n\nWe have successfully processed our Sentiment column( target); now, it’s time to process our input text data using a tokenizer.\nGetting transformers Package\n\nyou need to install the transformers package and then import it.\n\n!pip install transformers\nimport transformers\n\nLoading Model and Tokenizer from the transformers package \n\nfrom transformers import AutoTokenizer,TFBertModel\ntokenizer = AutoTokenizer.from_pretrained(‘bert-base-cased’)\nbert = TFBertModel.from_pretrained(‘bert-base-cased’)\n\nWe need a tokenizer to convert the input text’s word into tokens.\n\nThe classAutoTokenizer contains various types of tokenizers.\n\nTFBertModel pre-trained Bert model for TensorFlow.\n\nHere we are loading the bert-base-cased model.\n\n \nBert-Based-Case\nSource: Local\nInput Data Modeling\n\nBefore training, we need to convert the input textual data into BERT’s input data format using a tokenizer.\n\nSince we have loaded bert-base-cased, so tokenizer will also be Bert-base-cased.\n\n# Tokenize the input (takes some time) \n# here tokenizer using from bert-base-cased\nx_train = tokenizer(\n    text=df_train.Input.tolist(),\n    add_special_tokens=True,\n    max_length=70,\n    truncation=True,\n    padding=True, \n    return_tensors='tf',\n    return_token_type_ids = False,\n    return_attention_mask = True,\n    verbose = True)\nx_test = tokenizer(\n    text=df_test.Input.tolist(),\n    add_special_tokens=True,\n    max_length=70,\n    truncation=True,\n    padding=True, \n    return_tensors='tf',\n    return_token_type_ids = False,\n    return_attention_mask = True,\n    verbose = True)\n\nTokenizer takes all the necessary parameters and returns tensor in the same format Bert accepts.\n\n    return_token_type_ids = False: token_type_ids is not necessary for our training in this case.\n    return_attention_mask = True we want to include attention_mask in our input.\n    return_tensors=’tf’: we want our input tensor for the TensorFlow model.\n    max_length=70:\n    we want the maximum length of each sentence to be 70; if a sentence is\n    bigger than this, it will be trimmed if a sentence is smaller than\n    70 then it will be padded.\n    add_special_tokens=True, CLS, SEP token will be added in the tokenization.\n\nHereafter data modelling, the tokenizer will return a dictionary (x_train) containing ‘Input_ids’, ‘attention_mask’ as key for their respective\ndata.\n\ninput_ids = x_train[‘input_ids’]\nattention_mask = x_train['attention_mask']\n\n \nMulticlass Classification Using Transformers\nSource: Local\nModel Building\n\nImporting necessary libraries.\n\nimport tensorflow as tf\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.initializers import TruncatedNormal\nfrom tensorflow.keras.losses import CategoricalCrossentropy\nfrom tensorflow.keras.metrics import CategoricalAccuracy\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.layers import Input, Dense\n\nWe are using functional API to design our model.\n\nmax_len = 70\ninput_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\ninput_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"attention_mask\")\nembeddings = bert(input_ids,attention_mask = input_mask)[0] \nout = tf.keras.layers.GlobalMaxPool1D()(embeddings)\nout = Dense(128, activation='relu')(out)\nout = tf.keras.layers.Dropout(0.1)(out)\nout = Dense(32,activation = 'relu')(out)\ny = Dense(6,activation = 'sigmoid')(out)\nmodel = tf.keras.Model(inputs=[input_ids, input_mask], outputs=y)\nmodel.layers[2].trainable = True\n\nBert layers accept three input arrays, input_ids, attention_mask, token_type_ids\n\ninput_ids means our input words encoding, then attention mask,\n\ntoken_type_ids is necessary for the question-answering model; in this case, we will not pass token_type_ids.\n\n    For the Bert layer, we need two input layers, in this case, input_ids, attention_mask.\n    Embeddings contain hidden states of the Bert layer.\n    using\n    GlobalMaxPooling1D then dense layer to build CNN layers using hidden\n    states of Bert. These CNN layers will yield our output.\n\nbert[0] is the last hidden state, bert[1] is the\npooler_output, for building CNN layers on top of the BERT layer, we have\nused Bert’s hidden forms.\nModel Compilation\n\nDefining learning parameters and compiling the model.\n\noptimizer = Adam(\n    learning_rate=5e-05, # this learning rate is for bert model , taken from huggingface website \n    epsilon=1e-08,\n    decay=0.01,\n    clipnorm=1.0)\n# Set loss and metrics\nloss =CategoricalCrossentropy(from_logits = True)\nmetric = CategoricalAccuracy('balanced_accuracy'),\n# Compile the model\nmodel.compile(\n    optimizer = optimizer,\n    loss = loss, \n    metrics = metric)\n\n","metadata":{},"execution_count":null,"outputs":[]}]}