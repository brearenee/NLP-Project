{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Project Part 3\n\n[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/brearenee/NLP-Project/blob/main/part3.ipynb)\n","metadata":{}},{"cell_type":"markdown","source":"**NLP Problem:** Predicting the speaker from Star Trek: The Next Generation script lines for 8 main characters.\n\nIn this second phase of my project, I'm developing a deep learning model for this NLP task.\n\nAs learned in Part 1 and Part 2, the initial dataset's structure is less than ideal. To start Part 3, we must once again parse and clean the raw JSON data and transform it into a structured DataFrame.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport json\nimport requests\nurl = 'https://raw.githubusercontent.com/brearenee/NLP-Project/main/dataset/StarTrekDialogue_v2.json'\nresponse = requests.get(url)\n\n##This CodeBlock is thanks to ChatGPT :-) \nif response.status_code == 200:\n    json_data = json.loads(response.text)\n    lines = []\n    characters = []\n    episodes = []\n  \n    # extract the information from the JSON file for the \"TNG\" series\n    for series_name, series_data in json_data.items():\n        if series_name == \"TNG\": \n            for episode_name, episode_data in series_data.items():\n                for character_name, character_lines in episode_data.items():\n                    for line_text in character_lines:\n                        lines.append(line_text)\n                        characters.append(character_name)\n                        episodes.append(episode_name)\n                     \n    # Create a DataFrame from the extracted data\n    df = pd.DataFrame({\n        'Line': lines,\n        'Character': characters,\n    })\n\n    # Remove duplicate lines, keeping the first occurrence (preserving the original order)\n    df = df.drop_duplicates(subset='Line', keep='first')\n\n    # Reset the index of the DataFrame\n    df.reset_index(drop=True, inplace=True)\n\nelse:\n    print(f\"Failed to retrieve data. Status code: {response.status_code}\")\n    \n    \n##Remove Outliers (Characters with less than 1200 lines)\ncharacter_counts = df['Character'].value_counts()\ncharacters_to_remove = character_counts[character_counts < 1207].index\ndf = df[~df['Character'].isin(characters_to_remove)]\n\n##Print Value Count. \nprint(df['Character'].value_counts())\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-04T19:59:57.683475Z","iopub.execute_input":"2023-12-04T19:59:57.684481Z","iopub.status.idle":"2023-12-04T19:59:58.203475Z","shell.execute_reply.started":"2023-12-04T19:59:57.684441Z","shell.execute_reply":"2023-12-04T19:59:58.202498Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"Character\nPICARD     10798\nRIKER       6454\nDATA        5699\nLAFORGE     4111\nWORF        3185\nCRUSHER     2944\nTROI        2856\nName: count, dtype: int64\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# BERT","metadata":{}},{"cell_type":"code","source":"#https://www.analyticsvidhya.com/blog/2021/12/multiclass-classification-using-transformers/\n\n\n#Split the data \nfrom sklearn.model_selection import train_test_split\ntrain_df, val_df = train_test_split(df, test_size=0.2, random_state=20)\n\n#Converting our Character column into Categorical data\nencoded_dict = {'PICARD':0,'RIKER':1, 'DATA':2, 'LAFORGE':3, \n                'WORF':4, 'CRUSHER':5, 'TROI':6}\ntrain_df['Character'] = train_df.Character.map(encoded_dict)\nval_df['Character'] = val_df.Character.map(encoded_dict)\n\nprint(train_df['Character'].value_counts())\nprint(val_df['Character'].value_counts())\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-12-04T19:59:58.205584Z","iopub.execute_input":"2023-12-04T19:59:58.206000Z","iopub.status.idle":"2023-12-04T19:59:58.227481Z","shell.execute_reply.started":"2023-12-04T19:59:58.205963Z","shell.execute_reply":"2023-12-04T19:59:58.226405Z"},"trusted":true},"execution_count":52,"outputs":[{"name":"stdout","text":"Character\n0    8684\n1    5130\n2    4531\n3    3339\n4    2553\n5    2341\n6    2259\nName: count, dtype: int64\nCharacter\n0    2114\n1    1324\n2    1168\n3     772\n4     632\n5     603\n6     597\nName: count, dtype: int64\n","output_type":"stream"}]},{"cell_type":"code","source":"val_df.head(20)","metadata":{"execution":{"iopub.status.busy":"2023-12-04T19:59:58.228785Z","iopub.execute_input":"2023-12-04T19:59:58.229602Z","iopub.status.idle":"2023-12-04T19:59:58.239830Z","shell.execute_reply.started":"2023-12-04T19:59:58.229573Z","shell.execute_reply":"2023-12-04T19:59:58.238853Z"},"trusted":true},"execution_count":53,"outputs":[{"execution_count":53,"output_type":"execute_result","data":{"text/plain":"                                                    Line  Character\n16254                                              Nope.          1\n56245  I suspect the last thing Counsellor Troi would...          2\n9987   It's still running. The programme didn't\\r shu...          3\n2965   It seems to be a network of miniature circuitr...          3\n16825  We misinterpreted your actions as an attack on...          0\n35320              Peace envoy, in a stolen Vulcan ship.          0\n36705  Okay, we're going to track down any possible c...          3\n9177                 There was a moment when you smiled.          1\n11906                                          Which is?          1\n50132  Cellular peptides. That's exactly what the cre...          3\n4180                                          Champagne?          2\n21265  Open a hailing frequency. This is Captain Jean...          0\n46471  Thermal deflectors generate a field approximat...          2\n31350  Yes, you did, from computer twelve B nine, dec...          1\n20150               Captain, there has been an incident.          2\n36969  Life support failure. Decks nine, twelve, and ...          4\n1399                            Reduce to impulse power.          0\n14034               Is this part of your regular duties?          4\n43858                 Commander La Forge, please report.          2\n7060   We are getting out of here. Lieutenant Solis, ...          3","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Line</th>\n      <th>Character</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>16254</th>\n      <td>Nope.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>56245</th>\n      <td>I suspect the last thing Counsellor Troi would...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>9987</th>\n      <td>It's still running. The programme didn't\\r shu...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2965</th>\n      <td>It seems to be a network of miniature circuitr...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>16825</th>\n      <td>We misinterpreted your actions as an attack on...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>35320</th>\n      <td>Peace envoy, in a stolen Vulcan ship.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>36705</th>\n      <td>Okay, we're going to track down any possible c...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>9177</th>\n      <td>There was a moment when you smiled.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>11906</th>\n      <td>Which is?</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>50132</th>\n      <td>Cellular peptides. That's exactly what the cre...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4180</th>\n      <td>Champagne?</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>21265</th>\n      <td>Open a hailing frequency. This is Captain Jean...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>46471</th>\n      <td>Thermal deflectors generate a field approximat...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>31350</th>\n      <td>Yes, you did, from computer twelve B nine, dec...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>20150</th>\n      <td>Captain, there has been an incident.</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>36969</th>\n      <td>Life support failure. Decks nine, twelve, and ...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1399</th>\n      <td>Reduce to impulse power.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>14034</th>\n      <td>Is this part of your regular duties?</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>43858</th>\n      <td>Commander La Forge, please report.</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>7060</th>\n      <td>We are getting out of here. Lieutenant Solis, ...</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from tensorflow.keras.utils import to_categorical\n\ny_train = to_categorical(train_df.Character)\ny_test = to_categorical(val_df.Character)\n\n#We have successfully processed our Character column( target); \n#now, it’s time to process our input text data using a tokenizer.","metadata":{"execution":{"iopub.status.busy":"2023-12-04T19:59:58.242033Z","iopub.execute_input":"2023-12-04T19:59:58.242298Z","iopub.status.idle":"2023-12-04T19:59:58.250757Z","shell.execute_reply.started":"2023-12-04T19:59:58.242274Z","shell.execute_reply":"2023-12-04T19:59:58.249753Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"import transformers\n\n#Loading Model and Tokenizer from the transformers package \n\nfrom transformers import AutoTokenizer,TFBertModel\n#bert-base-uncased is another possible one\ntokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n#TFBertModel = pretrained BERT model for Tensor Flow\nbert = TFBertModel.from_pretrained('bert-base-uncased')\n\n#Input Data Modeling\n\n#Before training, we need to convert the input textual data into \n#BERT’s input data format using a tokenizer.\n#Since we have loaded bert-base-cased, \n#so tokenizer will also be Bert-base-cased.\n# Tokenize the input (takes some time) \n# here tokenizer using from bert-base-cased\nx_train = tokenizer(\n    text=train_df.Line.tolist(),\n    add_special_tokens=True,\n    max_length=40,\n    truncation=True,\n    padding=True, \n    return_tensors='tf',\n    return_token_type_ids = False,\n    return_attention_mask = True,\n    verbose = True)\nx_test = tokenizer(\n    text=val_df.Line.tolist(),\n    add_special_tokens=True,\n    max_length=40,\n    truncation=True,\n    padding=True, \n    return_tensors='tf',\n    return_token_type_ids = False,\n    return_attention_mask = True,\n    verbose = True)\n\n\n#Hereafter data modelling, the tokenizer will return a dictionary (x_train) containing ‘Input_ids’, ‘attention_mask’ as key for their respective\n#data.\n\ninput_ids = x_train['input_ids']\nattention_mask = x_train['attention_mask']","metadata":{"execution":{"iopub.status.busy":"2023-12-04T19:59:58.251870Z","iopub.execute_input":"2023-12-04T19:59:58.252176Z","iopub.status.idle":"2023-12-04T20:00:02.627571Z","shell.execute_reply.started":"2023-12-04T19:59:58.252152Z","shell.execute_reply":"2023-12-04T20:00:02.626731Z"},"trusted":true},"execution_count":55,"outputs":[{"name":"stderr","text":"Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nAll the weights of TFBertModel were initialized from the PyTorch model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Model Building","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.initializers import TruncatedNormal\nfrom tensorflow.keras.losses import CategoricalCrossentropy\nfrom tensorflow.keras.metrics import CategoricalAccuracy\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.layers import Input, Dense\n\nmax_len = 40\ninput_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\ninput_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"attention_mask\")\nembeddings = bert(input_ids,attention_mask = input_mask)[0] \nout = tf.keras.layers.GlobalMaxPool1D()(embeddings)\nout = Dense(128, activation='relu')(out)\nout = tf.keras.layers.Dropout(0.1)(out)\nout = Dense(64,activation = 'relu')(out)\ny = Dense(7,activation = 'softmax')(out)\nmodel = tf.keras.Model(inputs=[input_ids, input_mask], outputs=y)\nmodel.layers[3].trainable = True","metadata":{"execution":{"iopub.status.busy":"2023-12-04T20:00:02.629936Z","iopub.execute_input":"2023-12-04T20:00:02.630243Z","iopub.status.idle":"2023-12-04T20:00:04.743041Z","shell.execute_reply.started":"2023-12-04T20:00:02.630216Z","shell.execute_reply":"2023-12-04T20:00:04.742024Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"markdown","source":"# Model Compilation\n\nDefining learning parameters and compiling the model.","metadata":{}},{"cell_type":"code","source":"\noptimizer = tf.keras.optimizers.legacy.Adam(\n    learning_rate=4e-05, # 5e-05 is the learning rate is for bert model , taken from huggingface website \n    epsilon=1e-08,\n    decay=0.01,\n    clipnorm=1.0)\n# Set loss and metrics\nloss =CategoricalCrossentropy(from_logits = True)\nmetric = CategoricalAccuracy('balanced_accuracy'),\n# Compile the model\nmodel.compile(\n    optimizer = optimizer,\n    loss = loss, \n    metrics = metric)","metadata":{"execution":{"iopub.status.busy":"2023-12-04T20:00:04.744543Z","iopub.execute_input":"2023-12-04T20:00:04.745177Z","iopub.status.idle":"2023-12-04T20:00:04.772004Z","shell.execute_reply.started":"2023-12-04T20:00:04.745139Z","shell.execute_reply":"2023-12-04T20:00:04.771151Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"markdown","source":"# Model Training","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"#We have the model ready with x_train, y_train. You can now train the model.\ntrain_history = model.fit(\n    x ={'input_ids':x_train['input_ids'],'attention_mask':x_train['attention_mask']} ,\n    y = y_train,\n    validation_data = (\n    {'input_ids':x_test['input_ids'],'attention_mask':x_test['attention_mask']}, y_test\n    ),\n  epochs=4,\n    batch_size=32\n)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-04T20:00:04.773582Z","iopub.execute_input":"2023-12-04T20:00:04.773934Z","iopub.status.idle":"2023-12-04T20:18:24.961315Z","shell.execute_reply.started":"2023-12-04T20:00:04.773902Z","shell.execute_reply":"2023-12-04T20:18:24.960320Z"},"trusted":true},"execution_count":58,"outputs":[{"name":"stdout","text":"Epoch 1/4\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/backend.py:5562: UserWarning: \"`categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n  output, from_logits = _get_logits(\n","output_type":"stream"},{"name":"stdout","text":"902/902 [==============================] - 295s 302ms/step - loss: 1.4806 - balanced_accuracy: 0.4329 - val_loss: 1.3555 - val_balanced_accuracy: 0.4843\nEpoch 2/4\n902/902 [==============================] - 268s 297ms/step - loss: 1.3109 - balanced_accuracy: 0.5014 - val_loss: 1.3250 - val_balanced_accuracy: 0.4954\nEpoch 3/4\n902/902 [==============================] - 268s 298ms/step - loss: 1.2637 - balanced_accuracy: 0.5226 - val_loss: 1.3151 - val_balanced_accuracy: 0.4932\nEpoch 4/4\n902/902 [==============================] - 268s 297ms/step - loss: 1.2327 - balanced_accuracy: 0.5390 - val_loss: 1.3144 - val_balanced_accuracy: 0.4958\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Model Evaluation","metadata":{}},{"cell_type":"code","source":"import numpy as np\npredicted_raw = model.predict({'input_ids':x_test['input_ids'],'attention_mask':x_test['attention_mask']})\npredicted_raw[0]\ny_predicted = np.argmax(predicted_raw, axis = 1)\ny_true = val_df.Character\n\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_true, y_predicted))","metadata":{"execution":{"iopub.status.busy":"2023-12-04T20:18:24.963209Z","iopub.execute_input":"2023-12-04T20:18:24.964082Z","iopub.status.idle":"2023-12-04T20:18:48.616005Z","shell.execute_reply.started":"2023-12-04T20:18:24.964042Z","shell.execute_reply":"2023-12-04T20:18:48.615070Z"},"trusted":true},"execution_count":59,"outputs":[{"name":"stdout","text":"226/226 [==============================] - 23s 90ms/step\n              precision    recall  f1-score   support\n\n           0       0.55      0.62      0.58      2114\n           1       0.38      0.37      0.38      1324\n           2       0.60      0.69      0.64      1168\n           3       0.46      0.52      0.49       772\n           4       0.48      0.36      0.41       632\n           5       0.47      0.27      0.34       603\n           6       0.35      0.29      0.32       597\n\n    accuracy                           0.50      7210\n   macro avg       0.47      0.45      0.45      7210\nweighted avg       0.49      0.50      0.49      7210\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(pd_dtype):\n/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(pd_dtype):\n/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(pd_dtype):\n/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#1 1 epoch.  max_length = 70. second layer 34.  Accuracy = 42ish. \n\n#2 2 epoch. Max_length = 50? 40? second layer 64. Auucracy = .4797 \n\n#3 2 epoch Max_Length = 50 second layer = 64, removed Wesley\n              precision    recall  f1-score   support\n\n           0       0.52      0.70      0.60      2114\n           1       0.45      0.23      0.31      1324\n           2       0.60      0.70      0.65      1168\n           3       0.49      0.48      0.48       772\n           4       0.43      0.41      0.42       632\n           5       0.50      0.30      0.37       603\n           6       0.35      0.34      0.34       597\n\n    accuracy                           0.50      7210\n   macro avg       0.48      0.45      0.45      7210\nweighted avg       0.49      0.50      0.48      7210\n\n\n\n#4 Because my dataset is kind of small, i'm going to adjust where I start to fine tune the model. Right now I have it start at layer 2. I'm going to change that to 5.  And that worked well. \n\n902/902 [==============================] - 290s 297ms/step - loss: 1.4421 - balanced_accuracy: 0.4498 - val_loss: 1.3221 - val_balanced_accuracy: 0.4940\nEpoch 2/2\n902/902 [==============================] - 268s 297ms/step - loss: 1.2568 - balanced_accuracy: 0.5312 - val_loss: 1.2979 - val_balanced_accuracy: 0.5062\n226/226 [==============================] - 23s 88ms/step\n              precision    recall  f1-score   support\n\n           0       0.54      0.65      0.59      2114\n           1       0.39      0.39      0.39      1324\n           2       0.64      0.65      0.65      1168\n           3       0.49      0.51      0.50       772\n           4       0.46      0.43      0.44       632\n           5       0.46      0.29      0.35       603\n           6       0.41      0.25      0.31       597\n\n    accuracy                           0.51      7210\n   macro avg       0.48      0.45      0.46      7210\nweighted avg       0.50      0.51      0.50      7210\n\n\n\n#5 So lets add another epoch to this. \n226/226 [==============================] - 23s 89ms/step\n              precision    recall  f1-score   support\n\n           0       0.53      0.71      0.61      2114\n           1       0.42      0.30      0.35      1324\n           2       0.68      0.62      0.65      1168\n           3       0.49      0.52      0.50       772\n           4       0.47      0.46      0.47       632\n           5       0.48      0.31      0.38       603\n           6       0.33      0.31      0.32       597\n\n    accuracy                           0.51      7210\n   macro avg       0.49      0.46      0.47      7210\nweighted avg       0.50      0.51      0.50      7210\n\n902/902 [==============================] - 295s 302ms/step - loss: 1.4367 - balanced_accuracy: 0.4478 - val_loss: 1.3257 - val_balanced_accuracy: 0.4908\nEpoch 2/3\n902/902 [==============================] - 268s 298ms/step - loss: 1.2538 - balanced_accuracy: 0.5270 - val_loss: 1.3000 - val_balanced_accuracy: 0.5028\nEpoch 3/3\n902/902 [==============================] - 268s 298ms/step - loss: 1.1901 - balanced_accuracy: 0.5553 - val_loss: 1.2947 - val_balanced_accuracy: 0.5096\n\n#6 Lets change the fine tune layer to 7. \n902/902 [==============================] - 294s 302ms/step - loss: 1.4514 - balanced_accuracy: 0.4479 - val_loss: 1.3264 - val_balanced_accuracy: 0.4896\nEpoch 2/3\n902/902 [==============================] - 268s 297ms/step - loss: 1.2661 - balanced_accuracy: 0.5244 - val_loss: 1.3018 - val_balanced_accuracy: 0.4964\nEpoch 3/3\n902/902 [==============================] - 268s 297ms/step - loss: 1.2079 - balanced_accuracy: 0.5502 - val_loss: 1.2996 - val_balanced_accuracy: 0.5037\n226/226 [==============================] - 23s 89ms/step\n              precision    recall  f1-score   support\n\n           0       0.52      0.70      0.60      2114\n           1       0.40      0.36      0.38      1324\n           2       0.63      0.65      0.64      1168\n           3       0.50      0.47      0.48       772\n           4       0.52      0.36      0.43       632\n           5       0.46      0.32      0.38       603\n           6       0.36      0.24      0.29       597\n\n    accuracy                           0.50      7210\n   macro avg       0.48      0.44      0.46      7210\nweighted avg       0.49      0.50      0.49      7210\n\n","metadata":{}},{"cell_type":"markdown","source":"#7 Why Am I using sigmoid? Changing to softmax. \nEpoch 1/3\n\n/opt/conda/lib/python3.10/site-packages/keras/src/backend.py:5562: UserWarning: \"`categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n  output, from_logits = _get_logits(\n\n902/902 [==============================] - 294s 302ms/step - loss: 1.4475 - balanced_accuracy: 0.4466 - val_loss: 1.3240 - val_balanced_accuracy: 0.4907\nEpoch 2/3\n902/902 [==============================] - 268s 298ms/step - loss: 1.2492 - balanced_accuracy: 0.5335 - val_loss: 1.3032 - val_balanced_accuracy: 0.5064\nEpoch 3/3\n902/902 [==============================] - 268s 297ms/step - loss: 1.1846 - balanced_accuracy: 0.5579 - val_loss: 1.2991 - val_balanced_accuracy: 0.5098\n\nEpoch 1/3\n226/226 [==============================] - 23s 89ms/step\n              precision    recall  f1-score   support\n\n           0       0.52      0.72      0.61      2114\n           1       0.43      0.30      0.35      1324\n           2       0.66      0.63      0.65      1168\n           3       0.50      0.49      0.50       772\n           4       0.44      0.42      0.43       632\n           5       0.51      0.30      0.38       603\n           6       0.35      0.32      0.34       597\n\n    accuracy                           0.51      7210\n   macro avg       0.49      0.45      0.46      7210\nweighted avg       0.50      0.51      0.50      7210\n\n","metadata":{}},{"cell_type":"markdown","source":"#8? layers = 3\nepoch = 10\nlearning rage = 5e-06\nvery bad, canceled.  Let me try again with a different learning rate. \nlayers = 3\nepoch = 4\nlearning rage = 4e-05\n\n902/902 [==============================] - 295s 302ms/step - loss: 1.4806 - balanced_accuracy: 0.4329 - val_loss: 1.3555 - val_balanced_accuracy: 0.4843\nEpoch 2/4\n902/902 [==============================] - 268s 297ms/step - loss: 1.3109 - balanced_accuracy: 0.5014 - val_loss: 1.3250 - val_balanced_accuracy: 0.4954\nEpoch 3/4\n902/902 [==============================] - 268s 298ms/step - loss: 1.2637 - balanced_accuracy: 0.5226 - val_loss: 1.3151 - val_balanced_accuracy: 0.4932\nEpoch 4/4\n902/902 [==============================] - 268s 297ms/step - loss: 1.2327 - balanced_accuracy: 0.5390 - val_loss: 1.3144 - val_balanced_accuracy: 0.4958\n\n226/226 [==============================] - 23s 90ms/step\n              precision    recall  f1-score   support\n\n           0       0.55      0.62      0.58      2114\n           1       0.38      0.37      0.38      1324\n           2       0.60      0.69      0.64      1168\n           3       0.46      0.52      0.49       772\n           4       0.48      0.36      0.41       632\n           5       0.47      0.27      0.34       603\n           6       0.35      0.29      0.32       597\n\n    accuracy                           0.50      7210\n   macro avg       0.47      0.45      0.45      7210\nweighted avg       0.49      0.50      0.49      7210\n\n\n","metadata":{}}]}