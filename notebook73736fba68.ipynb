{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport json\nimport requests\nurl = 'https://raw.githubusercontent.com/brearenee/NLP-Project/main/dataset/StarTrekDialogue_v2.json'\nresponse = requests.get(url)\n\n##This CodeBlock is thanks to ChatGPT :-) \nif response.status_code == 200:\n    json_data = json.loads(response.text)\n    lines = []\n    characters = []\n    episodes = []\n  \n    # extract the information from the JSON file for the \"TNG\" series\n    for series_name, series_data in json_data.items():\n        if series_name == \"TNG\": \n            for episode_name, episode_data in series_data.items():\n                for character_name, character_lines in episode_data.items():\n                    for line_text in character_lines:\n                        lines.append(line_text)\n                        characters.append(character_name)\n                        episodes.append(episode_name)\n                     \n    # Create a DataFrame from the extracted data\n    df = pd.DataFrame({\n        'Line': lines,\n        'Character': characters,\n    })\n\n    # Remove duplicate lines, keeping the first occurrence (preserving the original order)\n    df = df.drop_duplicates(subset='Line', keep='first')\n\n    # Reset the index of the DataFrame\n    df.reset_index(drop=True, inplace=True)\n\nelse:\n    print(f\"Failed to retrieve data. Status code: {response.status_code}\")\n    \n    \n##Remove Outliers (Characters with less than 1000 lines)\ncharacter_counts = df['Character'].value_counts()\ncharacters_to_remove = character_counts[character_counts < 1000].index\ndf = df[~df['Character'].isin(characters_to_remove)]\n\n##Print Value Count. \nprint(df['Character'].value_counts())\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-04T05:43:57.524982Z","iopub.execute_input":"2023-12-04T05:43:57.525475Z","iopub.status.idle":"2023-12-04T05:43:58.107565Z","shell.execute_reply.started":"2023-12-04T05:43:57.525437Z","shell.execute_reply":"2023-12-04T05:43:58.106226Z"},"trusted":true},"execution_count":74,"outputs":[{"name":"stdout","text":"Character\nPICARD     10798\nRIKER       6454\nDATA        5699\nLAFORGE     4111\nWORF        3185\nCRUSHER     2944\nTROI        2856\nWESLEY      1206\nName: count, dtype: int64\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# BERT","metadata":{}},{"cell_type":"code","source":"#https://www.analyticsvidhya.com/blog/2021/12/multiclass-classification-using-transformers/\n\n\n#Split the data \nfrom sklearn.model_selection import train_test_split\ntrain_df, val_df = train_test_split(df, test_size=0.2, random_state=20)\n\n#Converting our Character column into Categorical data\nencoded_dict = {'PICARD':0,'RIKER':1, 'DATA':2, 'LAFORGE':3, \n                'WORF':4, 'CRUSHER':5, 'TROI':6,'WESLEY':7}\ntrain_df['Character'] = train_df.Character.map(encoded_dict)\nval_df['Character'] = val_df.Character.map(encoded_dict)\n\nprint(train_df['Character'].value_counts())\nprint(val_df['Character'].value_counts())\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-12-04T05:43:58.109442Z","iopub.execute_input":"2023-12-04T05:43:58.109784Z","iopub.status.idle":"2023-12-04T05:43:58.133302Z","shell.execute_reply.started":"2023-12-04T05:43:58.109755Z","shell.execute_reply":"2023-12-04T05:43:58.132129Z"},"trusted":true},"execution_count":75,"outputs":[{"name":"stdout","text":"Character\n0    8711\n1    5152\n2    4580\n3    3269\n4    2535\n5    2345\n6    2267\n7     943\nName: count, dtype: int64\nCharacter\n0    2087\n1    1302\n2    1119\n3     842\n4     650\n5     599\n6     589\n7     263\nName: count, dtype: int64\n","output_type":"stream"}]},{"cell_type":"code","source":"val_df.head(20)","metadata":{"execution":{"iopub.status.busy":"2023-12-04T05:43:58.134870Z","iopub.execute_input":"2023-12-04T05:43:58.135161Z","iopub.status.idle":"2023-12-04T05:43:58.145482Z","shell.execute_reply.started":"2023-12-04T05:43:58.135135Z","shell.execute_reply":"2023-12-04T05:43:58.144370Z"},"trusted":true},"execution_count":76,"outputs":[{"execution_count":76,"output_type":"execute_result","data":{"text/plain":"                                                    Line  Character\n20650  Q, get to the controls or get the hell out of ...          3\n20069          Why do you have all this anger toward me?          6\n15012  Interesting. Yes, that code hasn't been used i...          0\n44403                 No. Get ready to deploy the mines.          1\n28557  Ben Maxwell? But he's one of Starfleet's fines...          0\n30967  Mister Barclay, I want you to stop this experi...          0\n7991   Eighty five microvolts. Again.\\r Ninety. Again...          5\n39406                                         Forbid it?          0\n39909                          Picard to Bridge. Report.          0\n45915              What I mean is, it is not your fault.          4\n55766                                   I'm sure she is.          0\n41002                  And then to outlive them as well.          2\n7382               There goes the other shoe. They know.          0\n55061                         Well, that's only natural.          6\n16328  I've already taken the liberty of putting toge...          3\n28262  I believe you made an incorrect analysis of th...          2\n32144                   Increasing power to the shields.          1\n49832  Do these artefacts have some religious or cult...          1\n35930         What is it? What are you getting from him?          5\n33438  At Tanagra. A country? Tanagra on the ocean. A...          0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Line</th>\n      <th>Character</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>20650</th>\n      <td>Q, get to the controls or get the hell out of ...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>20069</th>\n      <td>Why do you have all this anger toward me?</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>15012</th>\n      <td>Interesting. Yes, that code hasn't been used i...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>44403</th>\n      <td>No. Get ready to deploy the mines.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>28557</th>\n      <td>Ben Maxwell? But he's one of Starfleet's fines...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>30967</th>\n      <td>Mister Barclay, I want you to stop this experi...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7991</th>\n      <td>Eighty five microvolts. Again.\\r Ninety. Again...</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>39406</th>\n      <td>Forbid it?</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>39909</th>\n      <td>Picard to Bridge. Report.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>45915</th>\n      <td>What I mean is, it is not your fault.</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>55766</th>\n      <td>I'm sure she is.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>41002</th>\n      <td>And then to outlive them as well.</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>7382</th>\n      <td>There goes the other shoe. They know.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>55061</th>\n      <td>Well, that's only natural.</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>16328</th>\n      <td>I've already taken the liberty of putting toge...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>28262</th>\n      <td>I believe you made an incorrect analysis of th...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>32144</th>\n      <td>Increasing power to the shields.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>49832</th>\n      <td>Do these artefacts have some religious or cult...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>35930</th>\n      <td>What is it? What are you getting from him?</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>33438</th>\n      <td>At Tanagra. A country? Tanagra on the ocean. A...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from tensorflow.keras.utils import to_categorical\n\ny_train = to_categorical(train_df.Character)\ny_test = to_categorical(val_df.Character)\n\n#We have successfully processed our Character column( target); \n#now, it’s time to process our input text data using a tokenizer.","metadata":{"execution":{"iopub.status.busy":"2023-12-04T05:43:58.146834Z","iopub.execute_input":"2023-12-04T05:43:58.147168Z","iopub.status.idle":"2023-12-04T05:43:58.155837Z","shell.execute_reply.started":"2023-12-04T05:43:58.147141Z","shell.execute_reply":"2023-12-04T05:43:58.155082Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"import transformers\n\n#Loading Model and Tokenizer from the transformers package \n\nfrom transformers import AutoTokenizer,TFBertModel\n#bert-base-uncased is another possible one\ntokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n#TFBertModel = pretrained BERT model for Tensor Flow\nbert = TFBertModel.from_pretrained('bert-base-uncased')\n\n#Input Data Modeling\n\n#Before training, we need to convert the input textual data into \n#BERT’s input data format using a tokenizer.\n#Since we have loaded bert-base-cased, \n#so tokenizer will also be Bert-base-cased.\n# Tokenize the input (takes some time) \n# here tokenizer using from bert-base-cased\nx_train = tokenizer(\n    text=train_df.Line.tolist(),\n    add_special_tokens=True,\n    max_length=70,\n    truncation=True,\n    padding=True, \n    return_tensors='tf',\n    return_token_type_ids = False,\n    return_attention_mask = True,\n    verbose = True)\nx_test = tokenizer(\n    text=val_df.Line.tolist(),\n    add_special_tokens=True,\n    max_length=70,\n    truncation=True,\n    padding=True, \n    return_tensors='tf',\n    return_token_type_ids = False,\n    return_attention_mask = True,\n    verbose = True)\n\n\n#Hereafter data modelling, the tokenizer will return a dictionary (x_train) containing ‘Input_ids’, ‘attention_mask’ as key for their respective\n#data.\n\ninput_ids = x_train['input_ids']\nattention_mask = x_train['attention_mask']","metadata":{"execution":{"iopub.status.busy":"2023-12-04T05:43:58.158736Z","iopub.execute_input":"2023-12-04T05:43:58.159092Z","iopub.status.idle":"2023-12-04T05:44:02.101284Z","shell.execute_reply.started":"2023-12-04T05:43:58.159056Z","shell.execute_reply":"2023-12-04T05:44:02.100147Z"},"trusted":true},"execution_count":78,"outputs":[{"name":"stderr","text":"Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nAll the weights of TFBertModel were initialized from the PyTorch model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Model Building","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.initializers import TruncatedNormal\nfrom tensorflow.keras.losses import CategoricalCrossentropy\nfrom tensorflow.keras.metrics import CategoricalAccuracy\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.layers import Input, Dense\n\nmax_len = 70\ninput_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\ninput_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"attention_mask\")\nembeddings = bert(input_ids,attention_mask = input_mask)[0] \nout = tf.keras.layers.GlobalMaxPool1D()(embeddings)\nout = Dense(128, activation='relu')(out)\nout = tf.keras.layers.Dropout(0.1)(out)\nout = Dense(32,activation = 'relu')(out)\ny = Dense(8,activation = 'sigmoid')(out)\nmodel = tf.keras.Model(inputs=[input_ids, input_mask], outputs=y)\nmodel.layers[2].trainable = True","metadata":{"execution":{"iopub.status.busy":"2023-12-04T05:44:02.102502Z","iopub.execute_input":"2023-12-04T05:44:02.102809Z","iopub.status.idle":"2023-12-04T05:44:04.200399Z","shell.execute_reply.started":"2023-12-04T05:44:02.102783Z","shell.execute_reply":"2023-12-04T05:44:04.199535Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"markdown","source":"# Model Compilation\n\nDefining learning parameters and compiling the model.","metadata":{}},{"cell_type":"code","source":"\noptimizer = tf.keras.optimizers.legacy.Adam(\n    learning_rate=5e-05, # this learning rate is for bert model , taken from huggingface website \n    epsilon=1e-08,\n    decay=0.01,\n    clipnorm=1.0)\n# Set loss and metrics\nloss =CategoricalCrossentropy(from_logits = True)\nmetric = CategoricalAccuracy('balanced_accuracy'),\n# Compile the model\nmodel.compile(\n    optimizer = optimizer,\n    loss = loss, \n    metrics = metric)","metadata":{"execution":{"iopub.status.busy":"2023-12-04T05:44:04.201554Z","iopub.execute_input":"2023-12-04T05:44:04.201874Z","iopub.status.idle":"2023-12-04T05:44:04.222473Z","shell.execute_reply.started":"2023-12-04T05:44:04.201846Z","shell.execute_reply":"2023-12-04T05:44:04.221568Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"markdown","source":"# Model Training","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"#We have the model ready with x_train, y_train. You can now train the model.\ntrain_history = model.fit(\n    x ={'input_ids':x_train['input_ids'],'attention_mask':x_train['attention_mask']} ,\n    y = y_train,\n    validation_data = (\n    {'input_ids':x_test['input_ids'],'attention_mask':x_test['attention_mask']}, y_test\n    ),\n  epochs=1,\n    batch_size=36\n)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-04T05:44:04.223601Z","iopub.execute_input":"2023-12-04T05:44:04.223879Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/backend.py:5562: UserWarning: \"`categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n  output, from_logits = _get_logits(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Model Evaluation","metadata":{}},{"cell_type":"code","source":"predicted_raw = model.predict({'input_ids':x_test['input_ids'],'attention_mask':x_test['attention_mask']})\npredicted_raw[0]\ny_predicted = np.argmax(predicted_raw, axis = 1)\ny_true = df_test.Character\n\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_true, y_predicted))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}