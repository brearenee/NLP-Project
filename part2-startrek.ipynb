{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ffa2005",
   "metadata": {
    "papermill": {
     "duration": 0.004373,
     "end_time": "2023-11-26T18:15:59.168271",
     "exception": false,
     "start_time": "2023-11-26T18:15:59.163898",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Project Part 1\n",
    "\n",
    "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/brearenee/NLP-Project/blob/main/startrek.ipynb)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sgeinitz/https://github.com/brearenee/NLP-Project/blob/main/startrek.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fce3067e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-26T18:15:59.180563Z",
     "iopub.status.busy": "2023-11-26T18:15:59.179876Z",
     "iopub.status.idle": "2023-11-26T18:16:02.779826Z",
     "shell.execute_reply": "2023-11-26T18:16:02.778550Z"
    },
    "papermill": {
     "duration": 3.608285,
     "end_time": "2023-11-26T18:16:02.782650",
     "exception": false,
     "start_time": "2023-11-26T18:15:59.174365",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "381379a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-26T18:16:02.793594Z",
     "iopub.status.busy": "2023-11-26T18:16:02.792226Z",
     "iopub.status.idle": "2023-11-26T18:16:06.036647Z",
     "shell.execute_reply": "2023-11-26T18:16:06.035636Z"
    },
    "papermill": {
     "duration": 3.252386,
     "end_time": "2023-11-26T18:16:06.039037",
     "exception": false,
     "start_time": "2023-11-26T18:16:02.786651",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/brearenee/NLP-Project/main/dataset/StarTrekDialogue_v2.json'\n",
    "response = requests.get(url)\n",
    "\n",
    "##This CodeBlock is thanks to ChatGPT :-) \n",
    "if response.status_code == 200:\n",
    "    json_data = json.loads(response.text)\n",
    "    lines = []\n",
    "    characters = []\n",
    "    episodes = []\n",
    "  \n",
    "    # extract the information from the JSON file for the \"TNG\" series\n",
    "    for series_name, series_data in json_data.items():\n",
    "        if series_name == \"TNG\": \n",
    "            for episode_name, episode_data in series_data.items():\n",
    "                for character_name, character_lines in episode_data.items():\n",
    "                    for line_text in character_lines:\n",
    "                        lines.append(line_text)\n",
    "                        characters.append(character_name)\n",
    "                        episodes.append(episode_name)\n",
    "                     \n",
    "    # Create a DataFrame from the extracted data\n",
    "    df = pd.DataFrame({\n",
    "        'Line': lines,\n",
    "        'Character': characters,\n",
    "        'Episode': episodes,\n",
    "    })\n",
    "\n",
    "    # Remove duplicate lines, keeping the first occurrence (preserving the original order)\n",
    "    df = df.drop_duplicates(subset='Line', keep='first')\n",
    "\n",
    "    # Reset the index of the DataFrame\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "else:\n",
    "    print(f\"Failed to retrieve data. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c4f8e1",
   "metadata": {
    "papermill": {
     "duration": 0.003468,
     "end_time": "2023-11-26T18:16:06.046501",
     "exception": false,
     "start_time": "2023-11-26T18:16:06.043033",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "As you can see, \"The Next Generation\" series has quite a few characters. Let's eliminate all of the outliers by removing characters that don't occur in more than 5 episodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45faa7d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-26T18:16:06.055918Z",
     "iopub.status.busy": "2023-11-26T18:16:06.055305Z",
     "iopub.status.idle": "2023-11-26T18:16:06.090125Z",
     "shell.execute_reply": "2023-11-26T18:16:06.088823Z"
    },
    "papermill": {
     "duration": 0.042231,
     "end_time": "2023-11-26T18:16:06.092415",
     "exception": false,
     "start_time": "2023-11-26T18:16:06.050184",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PICARD', 'DATA', 'TROI', 'WORF', 'Q', 'TASHA', \"O'BRIEN\", 'RIKER', 'WESLEY', 'CRUSHER', 'LAFORGE', 'COMPUTER', 'SECURITY', 'WOMAN', 'MAN', 'CREWMAN', 'CHIEF', 'MEDIC', 'VOICE', 'LWAXANA', 'CREWWOMAN', 'NURSE', 'GUINAN', 'PULASKI', 'ALL', 'OGAWA', 'KLINGON', 'ROMULAN', 'ALEXANDER', 'KEIKO', 'RO']\n"
     ]
    }
   ],
   "source": [
    "episode_counts = df.groupby('Character')['Episode'].nunique()\n",
    "\n",
    "characters_to_keep = episode_counts[episode_counts > 5].index\n",
    "\n",
    "df = df[df['Character'].isin(characters_to_keep)]\n",
    "unique_characters = df['Character'].unique().tolist()\n",
    "\n",
    "\n",
    "print(unique_characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1626a0c6",
   "metadata": {
    "papermill": {
     "duration": 0.003652,
     "end_time": "2023-11-26T18:16:06.100292",
     "exception": false,
     "start_time": "2023-11-26T18:16:06.096640",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This result is a lot better, but there are still some fields in here like \"ALL\" \"BOTH\" or \"GIRL\" that dont correlate to one single character/species \n",
    "\n",
    "Lets check the character distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "365845ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-26T18:16:06.109777Z",
     "iopub.status.busy": "2023-11-26T18:16:06.109406Z",
     "iopub.status.idle": "2023-11-26T18:16:06.122236Z",
     "shell.execute_reply": "2023-11-26T18:16:06.121002Z"
    },
    "papermill": {
     "duration": 0.020593,
     "end_time": "2023-11-26T18:16:06.124814",
     "exception": false,
     "start_time": "2023-11-26T18:16:06.104221",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Character\n",
       "PICARD       10798\n",
       "RIKER         6454\n",
       "DATA          5699\n",
       "LAFORGE       4111\n",
       "WORF          3185\n",
       "CRUSHER       2944\n",
       "TROI          2856\n",
       "WESLEY        1206\n",
       "Q              535\n",
       "PULASKI        487\n",
       "TASHA          474\n",
       "COMPUTER       471\n",
       "O'BRIEN        440\n",
       "GUINAN         432\n",
       "LWAXANA        404\n",
       "RO             304\n",
       "ALEXANDER      156\n",
       "OGAWA          110\n",
       "KEIKO           78\n",
       "CREWMAN         51\n",
       "WOMAN           46\n",
       "NURSE           30\n",
       "CHIEF           28\n",
       "VOICE           27\n",
       "ROMULAN         25\n",
       "MAN             22\n",
       "CREWWOMAN       17\n",
       "SECURITY        15\n",
       "KLINGON         14\n",
       "ALL             13\n",
       "MEDIC            6\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Character'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfebd86",
   "metadata": {
    "papermill": {
     "duration": 0.004104,
     "end_time": "2023-11-26T18:16:06.133230",
     "exception": false,
     "start_time": "2023-11-26T18:16:06.129126",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The chart provides a clear depiction of the number of lines attributed to each character in our dataset. Given the imbalance in this distribution, our next step involves addressing this by once again removing outliers.\n",
    "\n",
    "To achieve this, characters with fewer than 1000 lines will be removed. This allows us to retain characters that frequently appear, ensuring a substantial volume of lines for our model to gain insights and patterns from.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b54cb1e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-26T18:16:06.145435Z",
     "iopub.status.busy": "2023-11-26T18:16:06.144250Z",
     "iopub.status.idle": "2023-11-26T18:16:06.161761Z",
     "shell.execute_reply": "2023-11-26T18:16:06.160759Z"
    },
    "papermill": {
     "duration": 0.025852,
     "end_time": "2023-11-26T18:16:06.164319",
     "exception": false,
     "start_time": "2023-11-26T18:16:06.138467",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "character_counts = df['Character'].value_counts()\n",
    "\n",
    "characters_to_remove = character_counts[character_counts < 1000].index\n",
    "df = df[~df['Character'].isin(characters_to_remove)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6813050",
   "metadata": {
    "papermill": {
     "duration": 0.003839,
     "end_time": "2023-11-26T18:16:06.172634",
     "exception": false,
     "start_time": "2023-11-26T18:16:06.168795",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Tokenizing \n",
    "Before exploring our observations (lines), we'll initiate the tokenization process for the \"Lines\" field in our dataframe. This involves breaking down each sentence into individual tokens, enhancing the model's ability to interpret and analyze them.\n",
    "\n",
    "For the current phase, we're retaining stop words. There’s a chance they might hold stylistic nuances that are important for character prediction.  However, recognizing that our dataset is sourced from the internet, we'll enforce consistency by converting all text in the \"Line\" field to lowercase.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5d0e5e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-26T18:16:06.183633Z",
     "iopub.status.busy": "2023-11-26T18:16:06.182981Z",
     "iopub.status.idle": "2023-11-26T18:16:14.182426Z",
     "shell.execute_reply": "2023-11-26T18:16:14.181408Z"
    },
    "papermill": {
     "duration": 8.007721,
     "end_time": "2023-11-26T18:16:14.184971",
     "exception": false,
     "start_time": "2023-11-26T18:16:06.177250",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "df['Line'] = df['Line'].apply(lambda x: word_tokenize(x.lower()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12ee82c",
   "metadata": {
    "papermill": {
     "duration": 0.003967,
     "end_time": "2023-11-26T18:16:14.193366",
     "exception": false,
     "start_time": "2023-11-26T18:16:14.189399",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Lets print out the words/tokens that show up most. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9c659d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-26T18:16:14.204152Z",
     "iopub.status.busy": "2023-11-26T18:16:14.203190Z",
     "iopub.status.idle": "2023-11-26T18:16:14.365895Z",
     "shell.execute_reply": "2023-11-26T18:16:14.364899Z"
    },
    "papermill": {
     "duration": 0.170649,
     "end_time": "2023-11-26T18:16:14.368331",
     "exception": false,
     "start_time": "2023-11-26T18:16:14.197682",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Tokens:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Token\n",
       ".       52781\n",
       ",       25925\n",
       "the     19638\n",
       "to      15146\n",
       "i       14973\n",
       "you     12713\n",
       "?        9671\n",
       "a        9294\n",
       "it       8228\n",
       "of       7937\n",
       "is       7069\n",
       "that     6707\n",
       "we       6562\n",
       "'s       6159\n",
       "and      5186\n",
       "in       4767\n",
       "have     4663\n",
       "this     3921\n",
       "be       3784\n",
       "do       3759\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tokens = [token for sublist in df['Line'] for token in sublist]\n",
    "\n",
    "word_frequencies = pd.DataFrame({\n",
    "    'Token': all_tokens\n",
    "})\n",
    "\n",
    "top_words = word_frequencies['Token'].value_counts().head(20)\n",
    "\n",
    "print(\"Top 20 Tokens:\")\n",
    "top_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e011573f",
   "metadata": {
    "papermill": {
     "duration": 0.004279,
     "end_time": "2023-11-26T18:16:14.377219",
     "exception": false,
     "start_time": "2023-11-26T18:16:14.372940",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Upon inspection, you can see there’s a large presence of punctuation in our text. While these tokens are likely unhelpful, there’s a chance tokens like commas or question marks could contribute to speaking styles, enhancing character prediction in our models.  Because of this, punctuation will be retained in our dataset (for the time being)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6517aa",
   "metadata": {
    "papermill": {
     "duration": 0.004185,
     "end_time": "2023-11-26T18:16:14.385907",
     "exception": false,
     "start_time": "2023-11-26T18:16:14.381722",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "What's the vocabulary size? "
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30558,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 20.114609,
   "end_time": "2023-11-26T18:16:15.211527",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-11-26T18:15:55.096918",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
