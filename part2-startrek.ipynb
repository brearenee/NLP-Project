{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba4fc6e2",
   "metadata": {
    "papermill": {
     "duration": 0.0044,
     "end_time": "2023-11-26T18:30:22.465811",
     "exception": false,
     "start_time": "2023-11-26T18:30:22.461411",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Project Part 2\n",
    "\n",
    "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/brearenee/NLP-Project/blob/main/part2-startrek.ipynb)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sgeinitz/https://github.com/brearenee/NLP-Project/blob/main/part2-startrek.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72dd2da1",
   "metadata": {
    "papermill": {
     "duration": 0.005972,
     "end_time": "2023-11-26T18:30:22.476004",
     "exception": false,
     "start_time": "2023-11-26T18:30:22.470032",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**NLP Problem:** given a script from Star Trek The Next Generation, predict from 8 main characters who said a line. \n",
    "\n",
    "Part 2 of my project involves creating a basic model for my NLP problem\n",
    "\n",
    "\n",
    "As mentioned in Part 1, my dataset's original format is not in the most useful form.  \n",
    "To start Part 2, I must parse through the raw JSON and return an organized dataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b1ca595",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-26T18:30:22.486349Z",
     "iopub.status.busy": "2023-11-26T18:30:22.485875Z",
     "iopub.status.idle": "2023-11-26T18:30:25.316698Z",
     "shell.execute_reply": "2023-11-26T18:30:25.315403Z"
    },
    "papermill": {
     "duration": 2.84031,
     "end_time": "2023-11-26T18:30:25.320303",
     "exception": false,
     "start_time": "2023-11-26T18:30:22.479993",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c00880dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-26T18:30:25.333412Z",
     "iopub.status.busy": "2023-11-26T18:30:25.332445Z",
     "iopub.status.idle": "2023-11-26T18:30:28.317037Z",
     "shell.execute_reply": "2023-11-26T18:30:28.315818Z"
    },
    "papermill": {
     "duration": 2.994219,
     "end_time": "2023-11-26T18:30:28.320044",
     "exception": false,
     "start_time": "2023-11-26T18:30:25.325825",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/brearenee/NLP-Project/main/dataset/StarTrekDialogue_v2.json'\n",
    "response = requests.get(url)\n",
    "\n",
    "##This CodeBlock is thanks to ChatGPT :-) \n",
    "if response.status_code == 200:\n",
    "    json_data = json.loads(response.text)\n",
    "    lines = []\n",
    "    characters = []\n",
    "    episodes = []\n",
    "  \n",
    "    # extract the information from the JSON file for the \"TNG\" series\n",
    "    for series_name, series_data in json_data.items():\n",
    "        if series_name == \"TNG\": \n",
    "            for episode_name, episode_data in series_data.items():\n",
    "                for character_name, character_lines in episode_data.items():\n",
    "                    for line_text in character_lines:\n",
    "                        lines.append(line_text)\n",
    "                        characters.append(character_name)\n",
    "                        episodes.append(episode_name)\n",
    "                     \n",
    "    # Create a DataFrame from the extracted data\n",
    "    df = pd.DataFrame({\n",
    "        'Line': lines,\n",
    "        'Character': characters,\n",
    "        'Episode': episodes,\n",
    "    })\n",
    "\n",
    "    # Remove duplicate lines, keeping the first occurrence (preserving the original order)\n",
    "    df = df.drop_duplicates(subset='Line', keep='first')\n",
    "\n",
    "    # Reset the index of the DataFrame\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "else:\n",
    "    print(f\"Failed to retrieve data. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09272f3",
   "metadata": {
    "papermill": {
     "duration": 0.003592,
     "end_time": "2023-11-26T18:30:28.328146",
     "exception": false,
     "start_time": "2023-11-26T18:30:28.324554",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We then need to clean our dataset by removing non-main characters.  We are going to remove all characters that have less than 1000 lines. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7be1c1c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-26T18:30:28.337278Z",
     "iopub.status.busy": "2023-11-26T18:30:28.336810Z",
     "iopub.status.idle": "2023-11-26T18:30:28.365526Z",
     "shell.execute_reply": "2023-11-26T18:30:28.364323Z"
    },
    "papermill": {
     "duration": 0.036379,
     "end_time": "2023-11-26T18:30:28.368265",
     "exception": false,
     "start_time": "2023-11-26T18:30:28.331886",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "character_counts = df['Character'].value_counts()\n",
    "\n",
    "characters_to_remove = character_counts[character_counts < 1000].index\n",
    "df = df[~df['Character'].isin(characters_to_remove)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d14b9680",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-26T18:30:28.377667Z",
     "iopub.status.busy": "2023-11-26T18:30:28.377231Z",
     "iopub.status.idle": "2023-11-26T18:30:28.393057Z",
     "shell.execute_reply": "2023-11-26T18:30:28.391869Z"
    },
    "papermill": {
     "duration": 0.023164,
     "end_time": "2023-11-26T18:30:28.395394",
     "exception": false,
     "start_time": "2023-11-26T18:30:28.372230",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Character\n",
       "PICARD     10798\n",
       "RIKER       6454\n",
       "DATA        5699\n",
       "LAFORGE     4111\n",
       "WORF        3185\n",
       "CRUSHER     2944\n",
       "TROI        2856\n",
       "WESLEY      1206\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Character'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8260c935",
   "metadata": {
    "papermill": {
     "duration": 0.003787,
     "end_time": "2023-11-26T18:30:28.403135",
     "exception": false,
     "start_time": "2023-11-26T18:30:28.399348",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Tokenizing \n",
    "Before creating our model, we need to initiate the tokenization process for the \"Lines\" field in our dataframe. This involves breaking down each sentence into individual tokens, enhancing the model's ability to interpret and analyze them.\n",
    "\n",
    "For the current phase, we're retaining stop words. Thereâ€™s a chance they might hold stylistic nuances that are important for character prediction.  However, recognizing that our dataset is sourced from the internet, we'll enforce consistency by converting all text in the \"Line\" field to lowercase.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cb4d2ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-26T18:30:28.413246Z",
     "iopub.status.busy": "2023-11-26T18:30:28.412375Z",
     "iopub.status.idle": "2023-11-26T18:30:40.136075Z",
     "shell.execute_reply": "2023-11-26T18:30:40.134819Z"
    },
    "papermill": {
     "duration": 11.732188,
     "end_time": "2023-11-26T18:30:40.139240",
     "exception": false,
     "start_time": "2023-11-26T18:30:28.407052",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "df['Line'] = df['Line'].apply(lambda x: word_tokenize(x.lower()))\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 22.06526,
   "end_time": "2023-11-26T18:30:40.967259",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-11-26T18:30:18.901999",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
