{"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"papermill":{"default_parameters":{},"duration":480.127641,"end_time":"2023-11-27T01:46:41.166624","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-11-27T01:38:41.038983","version":"2.4.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Project Part 3\n\n[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/brearenee/NLP-Project/blob/main/Part3-StarTrek.ipynb)\n\n","metadata":{"papermill":{"duration":0.007126,"end_time":"2023-11-27T01:38:44.623427","exception":false,"start_time":"2023-11-27T01:38:44.616301","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"**NLP Problem:** Predicting the speaker from Star Trek: The Next Generation script lines for 8 main characters.\n\nIn this second phase of my project, I'm developing a deep learning model for this NLP task.\n\nAs learned in Part 1 and Part 2, the initial dataset's structure is less than ideal. To start Part 3, we must once again parse and clean the raw JSON data and transform it into a structured DataFrame.","metadata":{"papermill":{"duration":0.00647,"end_time":"2023-11-27T01:38:44.638801","exception":false,"start_time":"2023-11-27T01:38:44.632331","status":"completed"},"tags":[]}},{"cell_type":"code","source":"#!pip install transformers\n#!pip install transformers pandas torch\n!pip install cuml\nimport pandas as pd\nimport json\nimport requests\n\n#from sklearn.model_selection import train_test_split\n#from cuml.preprocessing.model_selection import train_test_split\n#from cuml.metrics import accuracy_score\nfrom transformers import BertTokenizer\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom transformers import BertForSequenceClassification\nfrom transformers import AdamW\n#from sklearn.metrics import accuracy_score\nfrom torch.optim.lr_scheduler import StepLR\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.data.dataset import random_split\n\n","metadata":{"papermill":{"duration":2.680625,"end_time":"2023-11-27T01:38:47.326186","exception":false,"start_time":"2023-11-27T01:38:44.645561","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-12-04T04:22:08.563106Z","iopub.execute_input":"2023-12-04T04:22:08.563826Z","iopub.status.idle":"2023-12-04T04:22:14.128752Z","shell.execute_reply.started":"2023-12-04T04:22:08.563787Z","shell.execute_reply":"2023-12-04T04:22:14.125702Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Collecting cuml\n  Using cached cuml-0.6.1.post1.tar.gz (1.1 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hBuilding wheels for collected packages: cuml\n  Building wheel for cuml (setup.py) ... \u001b[?25lerror\n  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n  \n  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n  \u001b[31m╰─>\u001b[0m \u001b[31m[41 lines of output]\u001b[0m\n  \u001b[31m   \u001b[0m /opt/conda/lib/python3.10/site-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n  \u001b[31m   \u001b[0m !!\n  \u001b[31m   \u001b[0m \n  \u001b[31m   \u001b[0m         ********************************************************************************\n  \u001b[31m   \u001b[0m         Please avoid running ``setup.py`` directly.\n  \u001b[31m   \u001b[0m         Instead, use pypa/build, pypa/installer or other\n  \u001b[31m   \u001b[0m         standards-based tools.\n  \u001b[31m   \u001b[0m \n  \u001b[31m   \u001b[0m         See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n  \u001b[31m   \u001b[0m         ********************************************************************************\n  \u001b[31m   \u001b[0m \n  \u001b[31m   \u001b[0m !!\n  \u001b[31m   \u001b[0m   self.initialize_options()\n  \u001b[31m   \u001b[0m Traceback (most recent call last):\n  \u001b[31m   \u001b[0m   File \"<string>\", line 2, in <module>\n  \u001b[31m   \u001b[0m   File \"<pip-setuptools-caller>\", line 34, in <module>\n  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-yv3kd6qf/cuml_1494c495b98346fba67de53c8024e2b1/setup.py\", line 18, in <module>\n  \u001b[31m   \u001b[0m     setup(name=pkg,\n  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/setuptools/__init__.py\", line 107, in setup\n  \u001b[31m   \u001b[0m     return distutils.core.setup(**attrs)\n  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/setuptools/_distutils/core.py\", line 185, in setup\n  \u001b[31m   \u001b[0m     return run_commands(dist)\n  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/setuptools/_distutils/core.py\", line 201, in run_commands\n  \u001b[31m   \u001b[0m     dist.run_commands()\n  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/setuptools/_distutils/dist.py\", line 969, in run_commands\n  \u001b[31m   \u001b[0m     self.run_command(cmd)\n  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/setuptools/dist.py\", line 1233, in run_command\n  \u001b[31m   \u001b[0m     super().run_command(command)\n  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/setuptools/_distutils/dist.py\", line 988, in run_command\n  \u001b[31m   \u001b[0m     cmd_obj.run()\n  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wheel/bdist_wheel.py\", line 399, in run\n  \u001b[31m   \u001b[0m     self.run_command(\"install\")\n  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/setuptools/_distutils/cmd.py\", line 318, in run_command\n  \u001b[31m   \u001b[0m     self.distribution.run_command(command)\n  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/setuptools/dist.py\", line 1233, in run_command\n  \u001b[31m   \u001b[0m     super().run_command(command)\n  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/setuptools/_distutils/dist.py\", line 988, in run_command\n  \u001b[31m   \u001b[0m     cmd_obj.run()\n  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-yv3kd6qf/cuml_1494c495b98346fba67de53c8024e2b1/setup.py\", line 15, in run\n  \u001b[31m   \u001b[0m     raise Exception(long_description)\n  \u001b[31m   \u001b[0m Exception: Please install cuml via the rapidsai conda channel. See https://rapids.ai/start.html for instructions.\n  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n  \n  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n\u001b[31m  ERROR: Failed building wheel for cuml\u001b[0m\u001b[31m\n\u001b[0m\u001b[?25h  Running setup.py clean for cuml\nFailed to build cuml\n\u001b[31mERROR: Could not build wheels for cuml, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[10], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#from sklearn.model_selection import train_test_split\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcuml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcuml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BertTokenizer\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cuml'"],"ename":"ModuleNotFoundError","evalue":"No module named 'cuml'","output_type":"error"}]},{"cell_type":"code","source":"url = 'https://raw.githubusercontent.com/brearenee/NLP-Project/main/dataset/StarTrekDialogue_v2.json'\nresponse = requests.get(url)\n\n##This CodeBlock is thanks to ChatGPT :-) \nif response.status_code == 200:\n    json_data = json.loads(response.text)\n    lines = []\n    characters = []\n    episodes = []\n  \n    # extract the information from the JSON file for the \"TNG\" series\n    for series_name, series_data in json_data.items():\n        if series_name == \"TNG\": \n            for episode_name, episode_data in series_data.items():\n                for character_name, character_lines in episode_data.items():\n                    for line_text in character_lines:\n                        lines.append(line_text)\n                        characters.append(character_name)\n                        episodes.append(episode_name)\n                     \n    # Create a DataFrame from the extracted data\n    df = pd.DataFrame({\n        'Line': lines,\n        'Character': characters,\n    })\n\n    # Remove duplicate lines, keeping the first occurrence (preserving the original order)\n    df = df.drop_duplicates(subset='Line', keep='first')\n\n    # Reset the index of the DataFrame\n    df.reset_index(drop=True, inplace=True)\n\nelse:\n    print(f\"Failed to retrieve data. Status code: {response.status_code}\")\n    \n    \n##Remove Outliers (Characters with less than 1000 lines)\ncharacter_counts = df['Character'].value_counts()\ncharacters_to_remove = character_counts[character_counts < 1000].index\ndf = df[~df['Character'].isin(characters_to_remove)]\n\n##Print Value Count. \nprint(df['Character'].value_counts())\n","metadata":{"papermill":{"duration":2.932522,"end_time":"2023-11-27T01:38:50.265589","exception":false,"start_time":"2023-11-27T01:38:47.333067","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-12-04T04:22:14.130016Z","iopub.status.idle":"2023-12-04T04:22:14.130504Z","shell.execute_reply.started":"2023-12-04T04:22:14.130271Z","shell.execute_reply":"2023-12-04T04:22:14.130293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# BERT \nBidirectional Encoder Representations from Transformers. \n\nBecause this is a classification task,  BERT seems like a good choice for a pre-trained deep learning model. \n\n","metadata":{"papermill":{"duration":0.007135,"end_time":"2023-11-27T01:38:50.375612","exception":false,"start_time":"2023-11-27T01:38:50.368477","status":"completed"},"tags":[]}},{"cell_type":"code","source":"\n##Split the dataset\n#train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n\n\n# Convert the DataFrame to PyTorch tensors\nfeatures_tensor = torch.tensor(df['features'].tolist(), dtype=torch.float32)\nlabels_tensor = torch.tensor(df['labels'].tolist(), dtype=torch.long)\n\n# Combine features and labels into a single TensorDataset\ndataset = torch.utils.data.TensorDataset(features_tensor, labels_tensor)\n\n# Specify the sizes for training and validation sets\ntrain_size = int(0.8 * len(dataset))\nval_size = len(dataset) - train_size\n\n# Split the dataset into training and validation sets\ntrain_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n\n# Create DataLoader instances\nbatch_size = 16\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n\n\n\n# Initialize BERT tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n# Tokenize input sequences\ntrain_tokenized = tokenizer(list(train_df['Line']), padding=True, truncation=True, return_tensors='pt')\nval_tokenized = tokenizer(list(val_df['Line']), padding=True, truncation=True, return_tensors='pt')\n\n# Convert labels to tensor with the correct data type (Long)\ntrain_labels = torch.tensor(train_df['Character'].astype('category').cat.codes.values, dtype=torch.long)\nval_labels = torch.tensor(val_df['Character'].astype('category').cat.codes.values, dtype=torch.long)\n\n# Create TensorDatasets\ntrain_dataset = TensorDataset(\n    train_tokenized['input_ids'],\n    train_tokenized['attention_mask'],\n    train_labels\n)\n\nval_dataset = TensorDataset(\n    val_tokenized['input_ids'],\n    val_tokenized['attention_mask'],\n    val_labels\n)\n\n# Set up training parameters\nepochs = 3\nbatch_size = 25  \n\n# Create DataLoader\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n\nprint(\"Tensor Datasets and DataLoader created\")\n\n","metadata":{"execution":{"iopub.status.busy":"2023-12-04T04:22:14.132367Z","iopub.status.idle":"2023-12-04T04:22:14.132813Z","shell.execute_reply.started":"2023-12-04T04:22:14.132616Z","shell.execute_reply":"2023-12-04T04:22:14.132635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load pre-trained BERT model for sequence classification\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=8)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-04T04:22:14.135083Z","iopub.status.idle":"2023-12-04T04:22:14.135836Z","shell.execute_reply.started":"2023-12-04T04:22:14.135511Z","shell.execute_reply":"2023-12-04T04:22:14.135545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Optimizer and Loss Function\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=2e-5)\nprint(\"optimizer and loss function\")\nprint(\"train dataset\")\nprint(len(train_dataset))","metadata":{"execution":{"iopub.status.busy":"2023-12-04T04:22:14.137576Z","iopub.status.idle":"2023-12-04T04:22:14.138109Z","shell.execute_reply.started":"2023-12-04T04:22:14.137861Z","shell.execute_reply":"2023-12-04T04:22:14.137881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training Loop\naccumulation_steps = 4\nscheduler = StepLR(optimizer, step_size=1, gamma=0.9)\n\nfor epoch in range(epochs):\n    scheduler.step()\n    model.train()  # Set the model to training mode\n    total_loss = 0\n    print_count = 1\n    print(print_count)\n    print_count = print_count + 1\n    for i in range(0, len(train_dataset), batch_size):\n        batch_inputs = (\n            train_dataset.tensors[0][i:i+batch_size],\n            train_dataset.tensors[1][i:i+batch_size]\n        )\n        batch_labels = train_dataset.tensors[2][i:i+batch_size]\n\n        optimizer.zero_grad()\n        print(\"mid\")\n        outputs = model(*batch_inputs)\n        loss = criterion(outputs.logits, batch_labels)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n        \n        if (i + 1) % accumulation_steps == 0:\n            optimizer.step()\n            optimizer.zero_grad()\n\n    avg_loss = total_loss / (len(train_dataset) / batch_size)\n    print(f\"Epoch {epoch + 1}/{epochs}, Training Loss: {avg_loss}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-04T04:22:14.139415Z","iopub.status.idle":"2023-12-04T04:22:14.140558Z","shell.execute_reply.started":"2023-12-04T04:22:14.140240Z","shell.execute_reply":"2023-12-04T04:22:14.140275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# After training, you might want to evaluate on the validation set\nmodel.eval()  # Set the model to evaluation mode\nval_loss = 0\n\nwith torch.no_grad():\n    for i in range(0, len(val_dataset), batch_size):\n        batch_inputs = (\n            val_dataset.tensors[0][i:i+batch_size],\n            val_dataset.tensors[1][i:i+batch_size]\n        )\n        batch_labels = val_dataset.tensors[2][i:i+batch_size]\n\n        outputs = model(*batch_inputs)\n        loss = criterion(outputs.logits, batch_labels)\n        val_loss += loss.item()\n\navg_val_loss = val_loss / (len(val_dataset) / batch_size)\nprint(f\"Validation Loss: {avg_val_loss}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-04T04:22:14.142177Z","iopub.status.idle":"2023-12-04T04:22:14.142666Z","shell.execute_reply.started":"2023-12-04T04:22:14.142413Z","shell.execute_reply":"2023-12-04T04:22:14.142433Z"},"trusted":true},"execution_count":null,"outputs":[]}]}