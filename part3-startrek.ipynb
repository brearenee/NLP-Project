{"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"papermill":{"default_parameters":{},"duration":480.127641,"end_time":"2023-11-27T01:46:41.166624","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-11-27T01:38:41.038983","version":"2.4.0"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Project Part 3\n\n[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/brearenee/NLP-Project/blob/main/Part3-StarTrek.ipynb)\n\n","metadata":{"papermill":{"duration":0.007126,"end_time":"2023-11-27T01:38:44.623427","exception":false,"start_time":"2023-11-27T01:38:44.616301","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"**NLP Problem:** Predicting the speaker from Star Trek: The Next Generation script lines for 8 main characters.\n\nIn this second phase of my project, I'm developing a deep learning model for this NLP task.\n\nAs learned in Part 1 and Part 2, the initial dataset's structure is less than ideal. To start Part 3, we must once again parse and clean the raw JSON data and transform it into a structured DataFrame.","metadata":{"papermill":{"duration":0.00647,"end_time":"2023-11-27T01:38:44.638801","exception":false,"start_time":"2023-11-27T01:38:44.632331","status":"completed"},"tags":[]}},{"cell_type":"code","source":"#!pip install transformers\n#!pip install transformers pandas torch\nimport pandas as pd\nimport json\nimport requests\n\nfrom sklearn.model_selection import train_test_split\nfrom transformers import BertTokenizer\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom transformers import BertForSequenceClassification\nfrom transformers import AdamW\nfrom sklearn.metrics import accuracy_score\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\nif device.type == \"cuda\":\n    print(torch.cuda.get_device_name(0))\n    print(\"Memory Usage:\")\n    print(\"Allocated:\", round(torch.cuda.memory_allocated(0) / 1024 ** 3, 1), \"GB\")\n    print(\"Cached:   \", round(torch.cuda.memory_reserved(0) / 1024 ** 3, 1), \"GB\")\n\n\n","metadata":{"papermill":{"duration":2.680625,"end_time":"2023-11-27T01:38:47.326186","exception":false,"start_time":"2023-11-27T01:38:44.645561","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-12-04T03:51:16.792717Z","iopub.execute_input":"2023-12-04T03:51:16.793610Z","iopub.status.idle":"2023-12-04T03:51:16.808407Z","shell.execute_reply.started":"2023-12-04T03:51:16.793566Z","shell.execute_reply":"2023-12-04T03:51:16.807271Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Using device: cuda\nTesla T4\nMemory Usage:\nAllocated: 0.0 GB\nCached:    0.0 GB\n","output_type":"stream"}]},{"cell_type":"code","source":"url = 'https://raw.githubusercontent.com/brearenee/NLP-Project/main/dataset/StarTrekDialogue_v2.json'\nresponse = requests.get(url)\n\n##This CodeBlock is thanks to ChatGPT :-) \nif response.status_code == 200:\n    json_data = json.loads(response.text)\n    lines = []\n    characters = []\n    episodes = []\n  \n    # extract the information from the JSON file for the \"TNG\" series\n    for series_name, series_data in json_data.items():\n        if series_name == \"TNG\": \n            for episode_name, episode_data in series_data.items():\n                for character_name, character_lines in episode_data.items():\n                    for line_text in character_lines:\n                        lines.append(line_text)\n                        characters.append(character_name)\n                        episodes.append(episode_name)\n                     \n    # Create a DataFrame from the extracted data\n    df = pd.DataFrame({\n        'Line': lines,\n        'Character': characters,\n    })\n\n    # Remove duplicate lines, keeping the first occurrence (preserving the original order)\n    df = df.drop_duplicates(subset='Line', keep='first')\n\n    # Reset the index of the DataFrame\n    df.reset_index(drop=True, inplace=True)\n\nelse:\n    print(f\"Failed to retrieve data. Status code: {response.status_code}\")\n    \n    \n##Remove Outliers (Characters with less than 1000 lines)\ncharacter_counts = df['Character'].value_counts()\ncharacters_to_remove = character_counts[character_counts < 1000].index\ndf = df[~df['Character'].isin(characters_to_remove)]\n\n##Print Value Count. \nprint(df['Character'].value_counts())\n","metadata":{"papermill":{"duration":2.932522,"end_time":"2023-11-27T01:38:50.265589","exception":false,"start_time":"2023-11-27T01:38:47.333067","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-12-04T03:51:16.811086Z","iopub.execute_input":"2023-12-04T03:51:16.811750Z","iopub.status.idle":"2023-12-04T03:51:17.352461Z","shell.execute_reply.started":"2023-12-04T03:51:16.811713Z","shell.execute_reply":"2023-12-04T03:51:17.351482Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Character\nPICARD     10798\nRIKER       6454\nDATA        5699\nLAFORGE     4111\nWORF        3185\nCRUSHER     2944\nTROI        2856\nWESLEY      1206\nName: count, dtype: int64\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# BERT \nBidirectional Encoder Representations from Transformers. \n\nBecause this is a classification task,  BERT seems like a good choice for a pre-trained deep learning model. \n\n","metadata":{"papermill":{"duration":0.007135,"end_time":"2023-11-27T01:38:50.375612","exception":false,"start_time":"2023-11-27T01:38:50.368477","status":"completed"},"tags":[]}},{"cell_type":"code","source":"\n##Split the dataset\ntrain_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n\n\n# Initialize BERT tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n# Tokenize input sequences\ntrain_tokenized = tokenizer(list(train_df['Line']), padding=True, truncation=True, return_tensors='pt')\nval_tokenized = tokenizer(list(val_df['Line']), padding=True, truncation=True, return_tensors='pt')\n\n# Convert labels to tensor with the correct data type (Long)\ntrain_labels = torch.tensor(train_df['Character'].astype('category').cat.codes.values, dtype=torch.long)\nval_labels = torch.tensor(val_df['Character'].astype('category').cat.codes.values, dtype=torch.long)\n\n# Create TensorDatasets\ntrain_dataset = TensorDataset(\n    train_tokenized['input_ids'],\n    train_tokenized['attention_mask'],\n    train_labels\n)\n\nval_dataset = TensorDataset(\n    val_tokenized['input_ids'],\n    val_tokenized['attention_mask'],\n    val_labels\n)\n\n# Set up training parameters\nepochs = 3\nbatch_size = 25  \n\n# Create DataLoader\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n\nprint(\"Tensor Datasets and DataLoader created\")\n\n","metadata":{"execution":{"iopub.status.busy":"2023-12-04T03:51:17.354100Z","iopub.execute_input":"2023-12-04T03:51:17.354423Z","iopub.status.idle":"2023-12-04T03:51:48.282146Z","shell.execute_reply.started":"2023-12-04T03:51:17.354393Z","shell.execute_reply":"2023-12-04T03:51:48.281228Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Tensor Datasets and DataLoader created\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load pre-trained BERT model for sequence classification\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=8)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-04T03:51:48.283334Z","iopub.execute_input":"2023-12-04T03:51:48.283705Z","iopub.status.idle":"2023-12-04T03:51:49.280584Z","shell.execute_reply.started":"2023-12-04T03:51:48.283668Z","shell.execute_reply":"2023-12-04T03:51:49.279541Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"#Optimizer and Loss Function\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=2e-5)\nprint(\"optimizer and loss function\")","metadata":{"execution":{"iopub.status.busy":"2023-12-04T03:51:49.282979Z","iopub.execute_input":"2023-12-04T03:51:49.283533Z","iopub.status.idle":"2023-12-04T03:51:49.303825Z","shell.execute_reply.started":"2023-12-04T03:51:49.283495Z","shell.execute_reply":"2023-12-04T03:51:49.302590Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"optimizer and loss function\n","output_type":"stream"}]},{"cell_type":"code","source":"# Training Loop\naccumulation_steps = 4\n\nfor epoch in range(epochs):\n    model.train()  # Set the model to training mode\n    total_loss = 0\n    print_count = 1\n    print(print_count)\n    print_count = print_count + 1\n    for i in range(0, len(train_dataset), batch_size):\n        batch_inputs = (\n            train_dataset.tensors[0][i:i+batch_size],\n            train_dataset.tensors[1][i:i+batch_size]\n        )\n        batch_labels = train_dataset.tensors[2][i:i+batch_size]\n\n        optimizer.zero_grad()\n\n        outputs = model(*batch_inputs)\n        loss = criterion(outputs.logits, batch_labels)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n        \n        if (i + 1) % accumulation_steps == 0:\n            optimizer.step()\n            optimizer.zero_grad()\n\n    avg_loss = total_loss / (len(train_dataset) / batch_size)\n    print(f\"Epoch {epoch + 1}/{epochs}, Training Loss: {avg_loss}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-04T03:51:49.305126Z","iopub.execute_input":"2023-12-04T03:51:49.305482Z","iopub.status.idle":"2023-12-04T03:52:34.778417Z","shell.execute_reply.started":"2023-12-04T03:51:49.305448Z","shell.execute_reply":"2023-12-04T03:52:34.777153Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"1\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[12], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39mbatch_inputs)\n\u001b[1;32m     20\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs\u001b[38;5;241m.\u001b[39mlogits, batch_labels)\n\u001b[0;32m---> 21\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     24\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"# After training, you might want to evaluate on the validation set\nmodel.eval()  # Set the model to evaluation mode\nval_loss = 0\n\nwith torch.no_grad():\n    for i in range(0, len(val_dataset), batch_size):\n        batch_inputs = (\n            val_dataset.tensors[0][i:i+batch_size],\n            val_dataset.tensors[1][i:i+batch_size]\n        )\n        batch_labels = val_dataset.tensors[2][i:i+batch_size]\n\n        outputs = model(*batch_inputs)\n        loss = criterion(outputs.logits, batch_labels)\n        val_loss += loss.item()\n\navg_val_loss = val_loss / (len(val_dataset) / batch_size)\nprint(f\"Validation Loss: {avg_val_loss}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-04T03:52:34.779507Z","iopub.status.idle":"2023-12-04T03:52:34.779924Z","shell.execute_reply.started":"2023-12-04T03:52:34.779746Z","shell.execute_reply":"2023-12-04T03:52:34.779764Z"},"trusted":true},"execution_count":null,"outputs":[]}]}