{"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"papermill":{"default_parameters":{},"duration":480.127641,"end_time":"2023-11-27T01:46:41.166624","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-11-27T01:38:41.038983","version":"2.4.0"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Project Part 3\n\n[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/brearenee/NLP-Project/blob/main/Part3-StarTrek.ipynb)\n\n","metadata":{"papermill":{"duration":0.007126,"end_time":"2023-11-27T01:38:44.623427","exception":false,"start_time":"2023-11-27T01:38:44.616301","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"**NLP Problem:** Predicting the speaker from Star Trek: The Next Generation script lines for 8 main characters.\n\nIn this second phase of my project, I'm developing a deep learning model for this NLP task.\n\nAs learned in Part 1 and Part 2, the initial dataset's structure is less than ideal. To start Part 3, we must once again parse and clean the raw JSON data and transform it into a structured DataFrame.","metadata":{"papermill":{"duration":0.00647,"end_time":"2023-11-27T01:38:44.638801","exception":false,"start_time":"2023-11-27T01:38:44.632331","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import pandas as pd\nimport json\nimport requests\n\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\nfrom torch.utils.data import DataLoader, TensorDataset, random_split\nimport torch\nfrom sklearn.preprocessing import LabelBinarizer\n","metadata":{"papermill":{"duration":2.680625,"end_time":"2023-11-27T01:38:47.326186","exception":false,"start_time":"2023-11-27T01:38:44.645561","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-12-04T01:41:48.546265Z","iopub.execute_input":"2023-12-04T01:41:48.546666Z","iopub.status.idle":"2023-12-04T01:41:48.552289Z","shell.execute_reply.started":"2023-12-04T01:41:48.546634Z","shell.execute_reply":"2023-12-04T01:41:48.551335Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"url = 'https://raw.githubusercontent.com/brearenee/NLP-Project/main/dataset/StarTrekDialogue_v2.json'\nresponse = requests.get(url)\n\n##This CodeBlock is thanks to ChatGPT :-) \nif response.status_code == 200:\n    json_data = json.loads(response.text)\n    lines = []\n    characters = []\n    episodes = []\n  \n    # extract the information from the JSON file for the \"TNG\" series\n    for series_name, series_data in json_data.items():\n        if series_name == \"TNG\": \n            for episode_name, episode_data in series_data.items():\n                for character_name, character_lines in episode_data.items():\n                    for line_text in character_lines:\n                        lines.append(line_text)\n                        characters.append(character_name)\n                        episodes.append(episode_name)\n                     \n    # Create a DataFrame from the extracted data\n    df = pd.DataFrame({\n        'Line': lines,\n        'Character': characters,\n    })\n\n    # Remove duplicate lines, keeping the first occurrence (preserving the original order)\n    df = df.drop_duplicates(subset='Line', keep='first')\n\n    # Reset the index of the DataFrame\n    df.reset_index(drop=True, inplace=True)\n\nelse:\n    print(f\"Failed to retrieve data. Status code: {response.status_code}\")\n    \n    \n##Remove Outliers (Characters with less than 1000 lines)\ncharacter_counts = df['Character'].value_counts()\ncharacters_to_remove = character_counts[character_counts < 1000].index\ndf = df[~df['Character'].isin(characters_to_remove)]\n\n##Print Value Count. \nprint(df['Character'].value_counts())\n","metadata":{"papermill":{"duration":2.932522,"end_time":"2023-11-27T01:38:50.265589","exception":false,"start_time":"2023-11-27T01:38:47.333067","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-12-04T01:41:48.557557Z","iopub.execute_input":"2023-12-04T01:41:48.557841Z","iopub.status.idle":"2023-12-04T01:41:49.061781Z","shell.execute_reply.started":"2023-12-04T01:41:48.557817Z","shell.execute_reply":"2023-12-04T01:41:49.060843Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Character\nPICARD     10798\nRIKER       6454\nDATA        5699\nLAFORGE     4111\nWORF        3185\nCRUSHER     2944\nTROI        2856\nWESLEY      1206\nName: count, dtype: int64\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# BERT \nBidirectional Encoder Representations from Transformers. \n\nBecause this is a classification task,  BERT seems like a good choice for a pre-trained deep learning model. \n\n","metadata":{"papermill":{"duration":0.007135,"end_time":"2023-11-27T01:38:50.375612","exception":false,"start_time":"2023-11-27T01:38:50.368477","status":"completed"},"tags":[]}},{"cell_type":"code","source":"\n##Split the dataset\ntrain_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\ndef tokenize_text(df, tokenizer, max_length=128):\n    input_ids = []\n    attention_masks = []\n\n    for _, row in df.iterrows():\n        encoded_dict = tokenizer.encode_plus(\n            row['Line'],\n            add_special_tokens=True,\n            max_length=max_length,\n            truncation=True,  # Explicitly set truncation to True\n            padding='max_length',  # Use 'max_length' for padding\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n\n        input_ids.append(encoded_dict['input_ids'])\n        attention_masks.append(encoded_dict['attention_mask'])\n\n    input_ids = torch.cat(input_ids, dim=0)\n    attention_masks = torch.cat(attention_masks, dim=0)\n\n    return input_ids, attention_masks\n\ntrain_input_ids, train_attention_masks = tokenize_text(train_df, tokenizer)\nval_input_ids, val_attention_masks = tokenize_text(val_df, tokenizer)\n\n#one hot encode the labels\nlabel_binarizer = LabelBinarizer()\ntrain_labels_one_hot = label_binarizer.fit_transform(train_df['Character'])\nval_labels_one_hot = label_binarizer.transform(val_df['Character'])","metadata":{"execution":{"iopub.status.busy":"2023-12-04T01:41:49.063810Z","iopub.execute_input":"2023-12-04T01:41:49.064551Z","iopub.status.idle":"2023-12-04T01:42:23.497401Z","shell.execute_reply.started":"2023-12-04T01:41:49.064514Z","shell.execute_reply":"2023-12-04T01:42:23.496245Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(pd_dtype):\n/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(pd_dtype):\n/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(pd_dtype):\n/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(pd_dtype):\n/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(pd_dtype):\n/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(pd_dtype):\n/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(pd_dtype):\n/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(pd_dtype):\n/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(pd_dtype):\n/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(pd_dtype):\n/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n","output_type":"stream"}]},{"cell_type":"code","source":"#Create PyTorch DataLoaders for your training and validation sets:\n\ntrain_dataset = TensorDataset(train_input_ids, train_attention_masks, torch.tensor(train_labels_one_hot, dtype=torch.float32))\nval_dataset = TensorDataset(val_input_ids, val_attention_masks, torch.tensor(val_labels_one_hot, dtype=torch.float32))\n\nbatch_size = 25\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-04T01:42:23.498718Z","iopub.execute_input":"2023-12-04T01:42:23.499051Z","iopub.status.idle":"2023-12-04T01:42:23.509614Z","shell.execute_reply.started":"2023-12-04T01:42:23.499024Z","shell.execute_reply":"2023-12-04T01:42:23.508736Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"Fine Tune the Pre-Trained BERT model","metadata":{}},{"cell_type":"code","source":"#Fine-tune a pre-trained BERT model for sequence classification:\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(df['Character'].unique()))\noptimizer = AdamW(model.parameters(), lr=2e-5)\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_dataloader) * 3)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\nloss_fct = torch.nn.BCEWithLogitsLoss()\n\nepochs = 3\naccumulation_steps = 4\nfor epoch in range(epochs):\n    model.train()\n    total_loss = 0\n    for i, batch in enumerate(train_dataloader):\n        input_ids, attention_mask, labels = batch\n        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n\n        model.zero_grad()\n        outputs = model(input_ids, attention_mask=attention_mask)\n        loss = loss_fct(outputs.logits, labels)\n        loss = loss / accumulation_steps  # Adjust the loss\n\n        total_loss += loss.item()\n\n        loss.backward()\n\n        if (i + 1) % accumulation_steps == 0:\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            scheduler.step()\n            model.zero_grad()\n\n    average_loss = total_loss / len(train_dataloader)\n    print(f'Epoch {epoch + 1}/{epochs}, Average Training Loss: {average_loss}')","metadata":{"execution":{"iopub.status.busy":"2023-12-04T01:42:23.511574Z","iopub.execute_input":"2023-12-04T01:42:23.512165Z","iopub.status.idle":"2023-12-04T02:10:24.858391Z","shell.execute_reply.started":"2023-12-04T01:42:23.512137Z","shell.execute_reply":"2023-12-04T02:10:24.857412Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3, Average Training Loss: 0.09263644951387104\nEpoch 2/3, Average Training Loss: 0.07607410329785151\nEpoch 3/3, Average Training Loss: 0.07106457880798848\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Evaluate","metadata":{}},{"cell_type":"code","source":"model.eval()\nval_loss = 0\ncorrect_predictions = 0\n\nwith torch.no_grad():\n    for batch in val_dataloader:\n        input_ids, attention_mask, labels = batch\n        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n\n        outputs = model(input_ids, attention_mask=attention_mask)\n        logits = outputs.logits\n\n        # Calculate loss (if needed)\n        # loss = criterion(logits, labels)\n        # val_loss += loss.item()\n\n        # Convert logits to binary predictions\n        predicted_labels = (torch.sigmoid(logits) > 0.5).to(torch.float32)\n\n        # Calculate accuracy for each sample\n        correct_predictions += (predicted_labels == labels).all(dim=1).sum().item()\n\n# Calculate average loss and accuracy\naverage_val_loss = val_loss / len(val_dataloader)\naccuracy = correct_predictions / len(val_df)\n\nprint(f'Average Validation Loss: {average_val_loss}, Accuracy: {accuracy}')\n","metadata":{"execution":{"iopub.status.busy":"2023-12-04T02:12:48.009596Z","iopub.execute_input":"2023-12-04T02:12:48.010030Z","iopub.status.idle":"2023-12-04T02:13:36.200913Z","shell.execute_reply.started":"2023-12-04T02:12:48.010000Z","shell.execute_reply":"2023-12-04T02:13:36.199869Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Average Validation Loss: 0.0, Accuracy: 0.26828613608911556\n","output_type":"stream"}]}]}