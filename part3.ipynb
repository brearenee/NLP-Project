{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Project Part 3\n\n[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/brearenee/NLP-Project/blob/main/part3.ipynb)\n","metadata":{}},{"cell_type":"markdown","source":"**NLP Problem:** Predicting the speaker from Star Trek: The Next Generation script lines for 8 main characters.\n\nIn this third phase of my project, I'm developing a deep learning model for this NLP task.\n\nAs learned in Part 1 and Part 2, the initial dataset's structure is less than ideal. To start Part 3, we must once again parse and clean the raw JSON data and transform it into a structured DataFrame.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport json\nimport requests\nurl = 'https://raw.githubusercontent.com/brearenee/NLP-Project/main/dataset/StarTrekDialogue_v2.json'\nresponse = requests.get(url)\n\n##This CodeBlock is thanks to ChatGPT :-) \nif response.status_code == 200:\n    json_data = json.loads(response.text)\n    lines = []\n    characters = []\n    episodes = []\n  \n    # extract the information from the JSON file for the \"TNG\" series\n    for series_name, series_data in json_data.items():\n        if series_name == \"TNG\": \n            for episode_name, episode_data in series_data.items():\n                for character_name, character_lines in episode_data.items():\n                    for line_text in character_lines:\n                        lines.append(line_text)\n                        characters.append(character_name)\n                        episodes.append(episode_name)\n                     \n    # Create a DataFrame from the extracted data\n    df = pd.DataFrame({\n        'Line': lines,\n        'Character': characters,\n    })\n\n    # Remove duplicate lines, keeping the first occurrence (preserving the original order)\n    df = df.drop_duplicates(subset='Line', keep='first')\n\n    # Reset the index of the DataFrame\n    df.reset_index(drop=True, inplace=True)\n\nelse:\n    print(f\"Failed to retrieve data. Status code: {response.status_code}\")\n    \n    \n##Remove Outliers (Characters with less than 1200 lines)\ncharacter_counts = df['Character'].value_counts()\ncharacters_to_remove = character_counts[character_counts < 1207].index\ndf = df[~df['Character'].isin(characters_to_remove)]\n\n\n##Print Value Count. \nprint(df['Character'].value_counts())\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-05T06:04:59.529481Z","iopub.execute_input":"2023-12-05T06:04:59.530432Z","iopub.status.idle":"2023-12-05T06:05:00.125916Z","shell.execute_reply.started":"2023-12-05T06:04:59.530394Z","shell.execute_reply":"2023-12-05T06:05:00.124905Z"},"trusted":true},"execution_count":86,"outputs":[{"name":"stdout","text":"Character\nPICARD     10798\nRIKER       6454\nDATA        5699\nLAFORGE     4111\nWORF        3185\nCRUSHER     2944\nTROI        2856\nName: count, dtype: int64\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# BERT\n\nI chose to use a Bidirectional Encoder Representations from Transformers model for my task.  BERT captures complex semantic relationships between words and entities in a sentence. This is beneficial for classification tasks involving natural language understanding.\n\nI start with this pre-trained model and fine tune it to better fit my data. ","metadata":{}},{"cell_type":"markdown","source":"### Preprocessing\n","metadata":{}},{"cell_type":"code","source":"#Help with this notebook was taken from the following tutorial: \n#https://www.analyticsvidhya.com/blog/2021/12/multiclass-classification-using-transformers/\n\n##Remove Riker because model is having a hard time. \ndf= df[df['Character'] != 'RIKER']\n\n#Split the data \nfrom sklearn.model_selection import train_test_split\ntrain_df, val_df = train_test_split(df, test_size=0.2, random_state=20)\n\n#Converting our Character column into Categorical data\n#encoded_dict = {'PICARD':0,'RIKER':1, 'DATA':2, 'LAFORGE':3, \n                #'WORF':4, 'CRUSHER':5, 'TROI':6 }\n    \n\nencoded_dict = {'PICARD':0,'DATA':1, 'LAFORGE':2, \n                'WORF':3, 'CRUSHER':4, 'TROI':5 }\n\n\ntrain_df['Character'] = train_df.Character.map(encoded_dict)\nval_df['Character'] = val_df.Character.map(encoded_dict)\n\nfrom tensorflow.keras.utils import to_categorical\n\n# Apply one-hot encoding to Categorical Data Labels\ny_train = to_categorical(train_df.Character)\ny_test = to_categorical(val_df.Character)\n\n#We have successfully processed our Character column (target/y); \n#Now, itâ€™s time to process our input text data using a tokenizer.\n\n","metadata":{"execution":{"iopub.status.busy":"2023-12-05T06:05:00.127967Z","iopub.execute_input":"2023-12-05T06:05:00.128262Z","iopub.status.idle":"2023-12-05T06:05:00.157151Z","shell.execute_reply.started":"2023-12-05T06:05:00.128236Z","shell.execute_reply":"2023-12-05T06:05:00.156115Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"markdown","source":"### Tokenizing\n","metadata":{}},{"cell_type":"code","source":"import transformers\nfrom transformers import AutoTokenizer,TFBertModel\n\n# bert-base-uncased is a smaller variants of the BERT model.\n# Using Hugging Face Transformers library to load \n# pre-trained tokenizer associated with the bert-base-uncased model\ntokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n\n# TFBertModel = pretrained BERT model for Tensor Flow\nbert = TFBertModel.from_pretrained('bert-base-uncased')\n\n#Input Data Modeling\n#Tokenizing the input based on the previously loaded tokenizer. \nx_train = tokenizer(\n    text=train_df.Line.tolist(),\n    add_special_tokens=True,\n    # max_length based on average line length from Part 1\n    max_length=40,\n    truncation=True,\n    padding=True, \n    return_tensors='tf',\n    return_token_type_ids = False,\n    return_attention_mask = True,\n    verbose = True)\nx_test = tokenizer(\n    text=val_df.Line.tolist(),\n    add_special_tokens=True,\n    max_length=40,\n    truncation=True,\n    padding=True, \n    return_tensors='tf',\n    return_token_type_ids = False,\n    return_attention_mask = True,\n    verbose = True)\n\n\n#The tokenized representation of the input text. \ninput_ids = x_train['input_ids']\n\n#Indicates which tokens in the input sequence should be attended to by the model and which ones should be ignored\nattention_mask = x_train['attention_mask']\n","metadata":{"execution":{"iopub.status.busy":"2023-12-05T06:05:00.158468Z","iopub.execute_input":"2023-12-05T06:05:00.158760Z","iopub.status.idle":"2023-12-05T06:05:03.708821Z","shell.execute_reply.started":"2023-12-05T06:05:00.158735Z","shell.execute_reply":"2023-12-05T06:05:03.707930Z"},"trusted":true},"execution_count":88,"outputs":[{"name":"stderr","text":"Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nAll the weights of TFBertModel were initialized from the PyTorch model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Model Building","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.initializers import TruncatedNormal\nfrom tensorflow.keras.losses import CategoricalCrossentropy\nfrom tensorflow.keras.metrics import CategoricalAccuracy\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.layers import Input, Dense\n\nmax_len = 40\n\n# Input_ids and attention_mask are given as input layers. \ninput_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\ninput_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"attention_mask\")\n\n# calling the bert model\nembeddings = bert(input_ids,attention_mask = input_mask)[0] \n# perform global max pooling on the input sequence\nout = tf.keras.layers.GlobalMaxPool1D()(embeddings)\n\n# A dense layer with 128 units and ReLU activation is applied\n# to the output of global max pooling.\nout = Dense(128, activation='relu')(out)\n\n# Dropout Layer\nout = tf.keras.layers.Dropout(0.2)(out)\n\n# More hidden layers\nout = Dense(64,activation = 'relu')(out)\n#out = Dense(32,activation = 'relu')(out)\n\n# Final output layer\ny = Dense(6,activation = 'softmax')(out)\n\n# Establish the architecture of our neural network,\n#connecting the input layers to the output layer.\nmodel = tf.keras.Model(inputs=[input_ids, input_mask], outputs=y)\n\n##  Allow updated weights for all layers after index 2\nmodel.layers[1].trainable = True","metadata":{"execution":{"iopub.status.busy":"2023-12-05T06:05:03.710813Z","iopub.execute_input":"2023-12-05T06:05:03.711116Z","iopub.status.idle":"2023-12-05T06:05:05.745375Z","shell.execute_reply.started":"2023-12-05T06:05:03.711092Z","shell.execute_reply":"2023-12-05T06:05:05.744486Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"markdown","source":"# Model Compilation\n\nDefinte the hyerparameters and compile the model","metadata":{}},{"cell_type":"code","source":"# Adam optimizer, used for updating the weights during training\noptimizer = tf.keras.optimizers.legacy.Adam(\n    learning_rate=5e-05, # 5e-05 is the learning rate is for bert model , taken from huggingface website \n    epsilon=1e-08,\n    decay=0.01,\n    clipnorm=1.0)\n\n# Categorical crossentropy loss function. \n# This is often used in multi-class classification problems where each sample can belong to only one class. \nloss =CategoricalCrossentropy(from_logits = False)\n\n# metric used to evaluate the model's performance during training/validation. \n# 'balanced_accuracy' indicates that it considered class imbalance in it's accuracy.\nmetric = CategoricalAccuracy('balanced_accuracy'),\n\n# Compile the model\nmodel.compile(\n    optimizer = optimizer,\n    loss = loss, \n    metrics = metric)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T06:05:05.746583Z","iopub.execute_input":"2023-12-05T06:05:05.746921Z","iopub.status.idle":"2023-12-05T06:05:05.769713Z","shell.execute_reply.started":"2023-12-05T06:05:05.746875Z","shell.execute_reply":"2023-12-05T06:05:05.768939Z"},"trusted":true},"execution_count":90,"outputs":[]},{"cell_type":"markdown","source":"# Model Training","metadata":{}},{"cell_type":"code","source":"#Train the model! \ntrain_history = model.fit(\n    x ={'input_ids':x_train['input_ids'],'attention_mask':x_train['attention_mask']} ,\n    y = y_train,\n    validation_data = (\n    {'input_ids':x_test['input_ids'],'attention_mask':x_test['attention_mask']}, y_test\n    ),\n    \n    \n  epochs=4,\n    batch_size=30\n)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-05T06:05:05.770727Z","iopub.execute_input":"2023-12-05T06:05:05.771024Z","iopub.status.idle":"2023-12-05T06:20:50.244844Z","shell.execute_reply.started":"2023-12-05T06:05:05.771001Z","shell.execute_reply":"2023-12-05T06:20:50.243946Z"},"trusted":true},"execution_count":91,"outputs":[{"name":"stdout","text":"Epoch 1/4\n790/790 [==============================] - 252s 291ms/step - loss: 1.2549 - balanced_accuracy: 0.5211 - val_loss: 1.1031 - val_balanced_accuracy: 0.5906\nEpoch 2/4\n790/790 [==============================] - 227s 287ms/step - loss: 1.0565 - balanced_accuracy: 0.6091 - val_loss: 1.0779 - val_balanced_accuracy: 0.6015\nEpoch 3/4\n790/790 [==============================] - 227s 287ms/step - loss: 0.9882 - balanced_accuracy: 0.6370 - val_loss: 1.0746 - val_balanced_accuracy: 0.5996\nEpoch 4/4\n790/790 [==============================] - 226s 287ms/step - loss: 0.9449 - balanced_accuracy: 0.6518 - val_loss: 1.0744 - val_balanced_accuracy: 0.6064\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Model Evaluation","metadata":{}},{"cell_type":"code","source":"# Run the predictions and see how we did. :-) \nimport numpy as np\npredicted_raw = model.predict({'input_ids':x_test['input_ids'],'attention_mask':x_test['attention_mask']})\npredicted_raw[0]\ny_predicted = np.argmax(predicted_raw, axis = 1)\ny_true = val_df.Character\n\n#Print results\n\nfrom sklearn.metrics import balanced_accuracy_score\nfrom sklearn.metrics import classification_report\n\nprint(classification_report(y_true, y_predicted))\n\npredictions_val = model.predict({'input_ids': x_test['input_ids'], 'attention_mask': x_test['attention_mask']})\ny_pred_val = predictions_val.argmax(axis=1)\n\n# Calculate balanced accuracy\nbalanced_acc = balanced_accuracy_score(y_true, y_predicted)\nprint(f'Balanced Accuracy: {balanced_acc:.4f}')","metadata":{"execution":{"iopub.status.busy":"2023-12-05T06:20:50.246414Z","iopub.execute_input":"2023-12-05T06:20:50.246728Z","iopub.status.idle":"2023-12-05T06:21:26.593085Z","shell.execute_reply.started":"2023-12-05T06:20:50.246702Z","shell.execute_reply":"2023-12-05T06:21:26.592259Z"},"trusted":true},"execution_count":92,"outputs":[{"name":"stdout","text":"185/185 [==============================] - 19s 88ms/step\n              precision    recall  f1-score   support\n\n           0       0.63      0.81      0.71      2146\n           1       0.68      0.61      0.64      1182\n           2       0.66      0.57      0.62       803\n           3       0.46      0.50      0.48       634\n           4       0.59      0.31      0.41       579\n           5       0.43      0.32      0.36       575\n\n    accuracy                           0.61      5919\n   macro avg       0.58      0.52      0.54      5919\nweighted avg       0.60      0.61      0.59      5919\n\n  1/185 [..............................] - ETA: 18s","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(pd_dtype):\n/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(pd_dtype):\n/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(pd_dtype):\n/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n","output_type":"stream"},{"name":"stdout","text":"185/185 [==============================] - 17s 90ms/step\nBalanced Accuracy: 0.5190\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(pd_dtype):\n/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(pd_dtype):\n/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(pd_dtype):\n/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\n# Calculate accuracy\naccuracy = accuracy_score(y_true, y_predicted)\nprint(f'Accuracy: {accuracy:.4f}')\n\n# Calculate confusion matrix\nconf_matrix = confusion_matrix(y_true, y_predicted)\nprint('Confusion Matrix:')\nprint(conf_matrix)\n\n# Display confusion matrix as a heatmap\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Confusion Matrix')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-05T06:21:26.594603Z","iopub.execute_input":"2023-12-05T06:21:26.594880Z","iopub.status.idle":"2023-12-05T06:21:26.744816Z","shell.execute_reply.started":"2023-12-05T06:21:26.594855Z","shell.execute_reply":"2023-12-05T06:21:26.743572Z"},"trusted":true},"execution_count":93,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[93], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Calculate accuracy\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(y_true, \u001b[43my_pred\u001b[49m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Calculate confusion matrix\u001b[39;00m\n","\u001b[0;31mNameError\u001b[0m: name 'y_pred' is not defined"],"ename":"NameError","evalue":"name 'y_pred' is not defined","output_type":"error"}]},{"cell_type":"markdown","source":"I spent a lot of time adjusting hyper parameters (epoch, batch size, max_length for tokens, learning rate, hidden layers) to try and increase my accuracy, but I kept overfitting. Unfortunately it doesn't want to budge past 60%.  \n\nI knew this task was going to be difficult when I started it.  My dataset is small and it's based on a TV script that relies on accents to distinguish characters and accents dont fully exist in text data. I love The Next Generation, but I wouldn't be able to tell the difference by looking at the script either.  I bet this 60% accuracy is significantly better than I could do.  \n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}