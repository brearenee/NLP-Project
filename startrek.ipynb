{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13fc9534",
   "metadata": {
    "papermill": {
     "duration": 0.003114,
     "end_time": "2023-11-08T18:16:24.474168",
     "exception": false,
     "start_time": "2023-11-08T18:16:24.471054",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Project Part 1\n",
    "\n",
    "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/sgeinitz/CS39AA-project/blob/main/project_part1.ipynb)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sgeinitz/CS39AA-project/blob/main/project_part1.ipynb)\n",
    "\n",
    "This notebook is intended to serve as a template to complete Part 1 of the projects. Feel free to modify this notebook as needed, but be sure to have the two main parts, a) a introductory proposal section describing what it is your doing to do and where the dataset originates, and b) an exploratory analysis section that has the histograms, charts, tables, etc. that are the output from your exploratory analysis. \n",
    "\n",
    "__Note you will want to remove the text above, and in the markdown cells below, and replace it with your own text describing the dataset, task, exploratory steps, etc.__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5da68e",
   "metadata": {
    "papermill": {
     "duration": 0.003938,
     "end_time": "2023-11-08T18:16:24.480524",
     "exception": false,
     "start_time": "2023-11-08T18:16:24.476586",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. Introduction/Background\n",
    "\n",
    "_In this section you will describe (in English) the dataset you are using as well as the NLP problem it deals with. For example, if you are planning to use the Twitter Natural Disaster dataset, then you will describe what the data and where it came as if you were explaining it to someone who does not know anything about the data. You will then describe how this is a __text classification__ problem, and that the labels are binary (e.g. a tweet either refers to a genuine/real natural disaster, or it does not)._ \n",
    "\n",
    "_Overall, this should be about a paragraph of text that could be read by someone outside of our class, and they could still understand what it is your project is doing._ \n",
    "\n",
    "_Note that you should __not__ simply write one sentence stating, \"This project is base on the Kaggle competition: Predicting Natural Disasters with Twitter._\"\n",
    "\n",
    "_If you are still looking for datasets to use, consider the following resources to explore text datasets._\n",
    "\n",
    "* https://huggingface.co/datasets/\n",
    "* https://www.kaggle.com/datasets\n",
    "* https://data-flair.training/blogs/machine-learning-datasets/ \n",
    "* https://pytorch.org/text/stable/datasets.html\n",
    "* https://github.com/niderhoff/nlp-datasets \n",
    "* https://medium.com/@ODSC/20-open-datasets-for-natural-language-processing-538fbfaf8e38 \n",
    "* https://imerit.net/blog/25-best-nlp-datasets-for-machine-learning-all-pbm/ \n",
    "\n",
    "\n",
    "_If you instead are planning to do a more research-oriented or applied type of project, then describe what it is that you plan to do._\n",
    "\n",
    "_If it is research, then what do you want to understand/explain better?_\n",
    "\n",
    "_If it is applied, then what it is you plan to build?_ \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a72d38",
   "metadata": {
    "papermill": {
     "duration": 0.002007,
     "end_time": "2023-11-08T18:16:24.484851",
     "exception": false,
     "start_time": "2023-11-08T18:16:24.482844",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Introduction \n",
    "\n",
    "So far, not quite sure how this is going to pan out. But my current idea is that given a sentence from a Star Trek script, I'd like to be able to predict who said it.  This is a text classification problem, but the output is categorical other than binary, as there are many cast members.   I'll work out the details soon. \n",
    "\n",
    "I'll be using the following dataset: \n",
    "https://www.kaggle.com/datasets/birkoruzicka/startrekdialoguetranscripts/data\n",
    "\n",
    "currently I'm learning how to parse the JSON file into a more useful format. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d682094",
   "metadata": {
    "papermill": {
     "duration": 0.001912,
     "end_time": "2023-11-08T18:16:24.488908",
     "exception": false,
     "start_time": "2023-11-08T18:16:24.486996",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2. Exploratory Data Analysis\n",
    "\n",
    "_You will now load the dataset and carry out some exploratory data analysis steps to better understand what text data looks like. See the examples from class on 10/. The following links provide some good resources of exploratory analyses of text data with Python._\n",
    "\n",
    "\n",
    "* https://neptune.ai/blog/exploratory-data-analysis-natural-language-processing-tools\n",
    "* https://regenerativetoday.com/exploratory-data-analysis-of-text-data-including-visualization-and-sentiment-analysis/\n",
    "* https://medium.com/swlh/text-summarization-guide-exploratory-data-analysis-on-text-data-4e22ce2dd6ad  \n",
    "* https://www.kdnuggets.com/2019/05/complete-exploratory-data-analysis-visualization-text-data.html  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6005cbf7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-08T18:16:24.495256Z",
     "iopub.status.busy": "2023-11-08T18:16:24.494618Z",
     "iopub.status.idle": "2023-11-08T18:16:24.856595Z",
     "shell.execute_reply": "2023-11-08T18:16:24.855618Z"
    },
    "papermill": {
     "duration": 0.368088,
     "end_time": "2023-11-08T18:16:24.859081",
     "exception": false,
     "start_time": "2023-11-08T18:16:24.490993",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import all of the python modules/packages you'll need here\n",
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360f4a62",
   "metadata": {
    "papermill": {
     "duration": 0.002019,
     "end_time": "2023-11-08T18:16:24.863730",
     "exception": false,
     "start_time": "2023-11-08T18:16:24.861711",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Take JSON dataset and parse it into a more suitable format/ dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39f29228",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-08T18:16:24.869800Z",
     "iopub.status.busy": "2023-11-08T18:16:24.869343Z",
     "iopub.status.idle": "2023-11-08T18:16:27.620299Z",
     "shell.execute_reply": "2023-11-08T18:16:27.619057Z"
    },
    "papermill": {
     "duration": 2.756612,
     "end_time": "2023-11-08T18:16:27.622560",
     "exception": false,
     "start_time": "2023-11-08T18:16:24.865948",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/brearenee/NLP-Project/main/dataset/StarTrekDialogue.json'\n",
    "\n",
    "# Make a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Load JSON data from the response\n",
    "    json_data = json.loads(response.text)\n",
    "\n",
    "    # Extract lines, characters, episodes, and series\n",
    "    lines = []\n",
    "    characters = []\n",
    "    episodes = []\n",
    "    series = []\n",
    "\n",
    "    # extract the information from the JSON file\n",
    "    for series_name, series_data in json_data.items():\n",
    "        for episode_name, episode_data in series_data.items():\n",
    "            for character_name, character_lines in episode_data.items():\n",
    "                for line_text in character_lines:\n",
    "                    lines.append(line_text)\n",
    "                    characters.append(character_name)\n",
    "                    episodes.append(episode_name)\n",
    "                    series.append(series_name)\n",
    "\n",
    "    # Create a DataFrame from the extracted data\n",
    "    df = pd.DataFrame({\n",
    "        'Line': lines,\n",
    "        'Character': characters,\n",
    "        'Episode': episodes,\n",
    "        'Series': series\n",
    "    })\n",
    "\n",
    "    # Remove duplicate lines, keeping the first occurrence (preserving the original order)\n",
    "    df = df.drop_duplicates(subset='Line', keep='first')\n",
    "\n",
    "    # Reset the index of the DataFrame\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "else:\n",
    "    print(f\"Failed to retrieve data. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a3cf01",
   "metadata": {
    "papermill": {
     "duration": 0.002023,
     "end_time": "2023-11-08T18:16:27.627112",
     "exception": false,
     "start_time": "2023-11-08T18:16:27.625089",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Test out our new Dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73b271cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-08T18:16:27.633404Z",
     "iopub.status.busy": "2023-11-08T18:16:27.632633Z",
     "iopub.status.idle": "2023-11-08T18:16:27.644292Z",
     "shell.execute_reply": "2023-11-08T18:16:27.643204Z"
    },
    "papermill": {
     "duration": 0.016901,
     "end_time": "2023-11-08T18:16:27.646181",
     "exception": false,
     "start_time": "2023-11-08T18:16:27.629280",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 Line Character  Episode  \\\n",
      "0                                  Check the circuit.     SPOCK  tos_000   \n",
      "1   It can't be the screen then. Definitely someth...     SPOCK  tos_000   \n",
      "2   Their call letters check with a survey expedit...     SPOCK  tos_000   \n",
      "3   Records show the Talos group has never been ex...     SPOCK  tos_000   \n",
      "4               We aren't going to go, to be certain?     SPOCK  tos_000   \n",
      "..                                                ...       ...      ...   \n",
      "95                                            Engage.      PIKE  tos_000   \n",
      "96                                            Yeoman.      PIKE  tos_000   \n",
      "97   I thought I told you that when I'm on the bridge      PIKE  tos_000   \n",
      "98                              Oh, I see. Thank you.      PIKE  tos_000   \n",
      "99  She does a good job, all right. It's just that...      PIKE  tos_000   \n",
      "\n",
      "   Series  \n",
      "0     TOS  \n",
      "1     TOS  \n",
      "2     TOS  \n",
      "3     TOS  \n",
      "4     TOS  \n",
      "..    ...  \n",
      "95    TOS  \n",
      "96    TOS  \n",
      "97    TOS  \n",
      "98    TOS  \n",
      "99    TOS  \n",
      "\n",
      "[100 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df.head(100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6.47405,
   "end_time": "2023-11-08T18:16:28.169258",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-11-08T18:16:21.695208",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
