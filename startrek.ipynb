{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6a0b92d",
   "metadata": {
    "papermill": {
     "duration": 0.002953,
     "end_time": "2023-11-11T15:49:32.641887",
     "exception": false,
     "start_time": "2023-11-11T15:49:32.638934",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Project Part 1\n",
    "\n",
    "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/brearenee/NLP-Project/blob/main/startrek.ipynb)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sgeinitz/https://github.com/brearenee/NLP-Project/blob/main/startrek.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8931abbc",
   "metadata": {
    "papermill": {
     "duration": 0.003848,
     "end_time": "2023-11-11T15:49:32.648364",
     "exception": false,
     "start_time": "2023-11-11T15:49:32.644516",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. Introduction/Background\n",
    "In this notebook, I'll be working with a dataset containing dialogue transcripts from various Star Trek series episodes, which I found on [kaggle](http://http://https://www.kaggle.com/datasets/birkoruzicka/startrekdialoguetranscripts/data). This dataset provides a large amount of script lines, each accompanied by information on episode, seriess, and the character who delivered said line. \n",
    "\n",
    "The objective of this project is to build a model capable of predicting the character associated with a given line from the script.  This type of problem is known as Speaker Identification and i'll be treating it as a text classification problem since model will need to learn patterns that are indicative of the speaking style of each character. Since there are multiple different characters which occur in this dataset, the end result is a multi-class classification task. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98eb9e2c",
   "metadata": {
    "papermill": {
     "duration": 0.002057,
     "end_time": "2023-11-11T15:49:32.652745",
     "exception": false,
     "start_time": "2023-11-11T15:49:32.650688",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2. Data Preprocessing\n",
    "\n",
    "The datasource I'm working with is initially structured as a highly nested JSON file, and it's original format isn't quite optimal for the model I'm trying to create.  Becase of this, I'll need to parse through the file and transform it's structure to allow for a more useful dataframe. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f40e4c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-11T15:49:32.659964Z",
     "iopub.status.busy": "2023-11-11T15:49:32.658882Z",
     "iopub.status.idle": "2023-11-11T15:49:33.733509Z",
     "shell.execute_reply": "2023-11-11T15:49:33.732350Z"
    },
    "papermill": {
     "duration": 1.081038,
     "end_time": "2023-11-11T15:49:33.736023",
     "exception": false,
     "start_time": "2023-11-11T15:49:32.654985",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import all of the python modules/packages you'll need here\n",
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7a943c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-11T15:49:33.743460Z",
     "iopub.status.busy": "2023-11-11T15:49:33.742837Z",
     "iopub.status.idle": "2023-11-11T15:49:36.663495Z",
     "shell.execute_reply": "2023-11-11T15:49:36.662570Z"
    },
    "papermill": {
     "duration": 2.927306,
     "end_time": "2023-11-11T15:49:36.666268",
     "exception": false,
     "start_time": "2023-11-11T15:49:33.738962",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/brearenee/NLP-Project/main/dataset/StarTrekDialogue.json'\n",
    "\n",
    "# Make a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Load JSON data from the response\n",
    "    json_data = json.loads(response.text)\n",
    "\n",
    "    # Extract lines, characters, episodes, and series\n",
    "    lines = []\n",
    "    characters = []\n",
    "    episodes = []\n",
    "    series = []\n",
    "\n",
    "    # extract the information from the JSON file\n",
    "    for series_name, series_data in json_data.items():\n",
    "        for episode_name, episode_data in series_data.items():\n",
    "            for character_name, character_lines in episode_data.items():\n",
    "                for line_text in character_lines:\n",
    "                    lines.append(line_text)\n",
    "                    characters.append(character_name)\n",
    "                    episodes.append(episode_name)\n",
    "                    series.append(series_name)\n",
    "\n",
    "    # Create a DataFrame from the extracted data\n",
    "    df = pd.DataFrame({\n",
    "        'Line': lines,\n",
    "        'Character': characters,\n",
    "        'Episode': episodes,\n",
    "        'Series': series\n",
    "    })\n",
    "\n",
    "    # Remove duplicate lines, keeping the first occurrence (preserving the original order)\n",
    "    df = df.drop_duplicates(subset='Line', keep='first')\n",
    "\n",
    "    # Reset the index of the DataFrame\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "else:\n",
    "    print(f\"Failed to retrieve data. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1959c2a",
   "metadata": {
    "papermill": {
     "duration": 0.002499,
     "end_time": "2023-11-11T15:49:36.671884",
     "exception": false,
     "start_time": "2023-11-11T15:49:36.669385",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Test out our new Dataframe**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5de678",
   "metadata": {
    "papermill": {
     "duration": 0.002518,
     "end_time": "2023-11-11T15:49:36.677482",
     "exception": false,
     "start_time": "2023-11-11T15:49:36.674964",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d7eebb8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-11T15:49:36.685061Z",
     "iopub.status.busy": "2023-11-11T15:49:36.684302Z",
     "iopub.status.idle": "2023-11-11T15:49:36.698393Z",
     "shell.execute_reply": "2023-11-11T15:49:36.696842Z"
    },
    "papermill": {
     "duration": 0.021066,
     "end_time": "2023-11-11T15:49:36.701264",
     "exception": false,
     "start_time": "2023-11-11T15:49:36.680198",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 Line Character  Episode  \\\n",
      "0                                  Check the circuit.     SPOCK  tos_000   \n",
      "1   It can't be the screen then. Definitely someth...     SPOCK  tos_000   \n",
      "2   Their call letters check with a survey expedit...     SPOCK  tos_000   \n",
      "3   Records show the Talos group has never been ex...     SPOCK  tos_000   \n",
      "4               We aren't going to go, to be certain?     SPOCK  tos_000   \n",
      "..                                                ...       ...      ...   \n",
      "95                                            Engage.      PIKE  tos_000   \n",
      "96                                            Yeoman.      PIKE  tos_000   \n",
      "97   I thought I told you that when I'm on the bridge      PIKE  tos_000   \n",
      "98                              Oh, I see. Thank you.      PIKE  tos_000   \n",
      "99  She does a good job, all right. It's just that...      PIKE  tos_000   \n",
      "\n",
      "   Series  \n",
      "0     TOS  \n",
      "1     TOS  \n",
      "2     TOS  \n",
      "3     TOS  \n",
      "4     TOS  \n",
      "..    ...  \n",
      "95    TOS  \n",
      "96    TOS  \n",
      "97    TOS  \n",
      "98    TOS  \n",
      "99    TOS  \n",
      "\n",
      "[100 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df.head(100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 8.575393,
   "end_time": "2023-11-11T15:49:37.225395",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-11-11T15:49:28.650002",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
